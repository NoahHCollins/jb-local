{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# the following allows us to plot triangles indicating convergence order\n",
    "from mpltools import annotation\n",
    "# as this lecture is about interpolation we will make use of this SciPy library\n",
    "import scipy.interpolate as si\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "# some default font sizes for plots\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Dejavu Sans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 9.1 Interpolation & Quadrature 2 <a class=\"tocSkip\">\n",
    "\n",
    "Lecture 9.1  \n",
    "Matt Piggott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan / learning objectives: <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "* Review and consolidation of some Computational Mathematics material - I will review quite a lot of key material we saw in that previous module.\n",
    "\n",
    "\n",
    "* To cover some more advanced topics related to the approximation of data and functions.\n",
    "\n",
    "\n",
    "* To introduce piecewise-linear basis functions and expansions (an important part of the finite element method).\n",
    "\n",
    "\n",
    "* To cover some more advanced topics related to the approximation of integrals.\n",
    "\n",
    "\n",
    "* More exposure to error analysis and convergence rates.\n",
    "\n",
    "\n",
    "* More practice of your coding skills.\n",
    "\n",
    "\n",
    "* Introduction to the generic principles of Richardson extrapolation and adaptive algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's really important here? <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "- General familiarity with errors and their dependence on the specifics of the numerical method, the function being considered and the interval size (or mesh resolution).\n",
    "\n",
    "\n",
    "- The specifics of the hat functions used as a basis for p/w linear interpolation.\n",
    "\n",
    "\n",
    "- Anything with a double star can be completely ignored, including the similarly coloured material within that section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation \n",
    "\n",
    "## Review\n",
    "\n",
    "Recall that we often wish to approximate data, e.g. in 1D this might be a series of $y$ values at $x$ locations:\n",
    "\n",
    "$$ (x_i, y_i),\\;\\;\\;\\;\\;\\; i=0,\\ldots,N,$$\n",
    "\n",
    "\n",
    "The data may be known to be exact (e.g. we may wish to approximate a complex function, which we can evaluate exactly, by a simpler expression say), or it may have acknowledged errors from measurement/observational techniques, with known or unknown [error bars](https://en.wikipedia.org/wiki/Error_bar).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "[*Interpolation*](https://en.wikipedia.org/wiki/Interpolation) generally assumes that these data points are *exact* (e.g. no measurement errors) and at *distinct* $x$ locations, i.e. there is no ambiguity in a mapping from $x$ to $y$ (which there would be if we had multiple $y$ values for the same $x$; we will see this scenario in the case of curve-fitting covered below. The same $x$ and $y$ pairs exactly repeated obviously means repeated data and depending on application all the exact replicas might be removed). \n",
    "\n",
    "<br>\n",
    "\n",
    "The requirement for distinct $x$ locations means that we have a constraint on the $x_i$'s which can be written as\n",
    "\n",
    "$$x_0 < x_1 < \\ldots < x_N,$$ \n",
    "\n",
    "[Note that sometimes we may have control over the $x$ locations, but sometimes we won't - we'll just be given arbitrary data - we'll cover both cases below].\n",
    "\n",
    "<br>\n",
    "\n",
    "The process of interpolation involves finding the function $f$ such that \n",
    "\n",
    "$$y_i = f(x_i),\\;\\;\\;\\;\\;\\; \\forall i$$\n",
    "\n",
    "\n",
    "and recall that we have a lot of choice for how we construct the interpolating or curve-fitting function $f$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some arbitrary test data\n",
    "\n",
    "Let's first invent a small set of arbitrary data which we shall seek to interpolate throughout this lecture using different methods, and define a function that will save us from typing the same plotting commands multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Invent some raw data - we will use the notation (xi,yi) for the\n",
    "# given data, where xi and yi are of length N+1 (N=len(xi)-1)\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# We will want to overlay a plot of the raw data a few times below so \n",
    "# let's do this via a function that we can call repeatedly\n",
    "# [Note that I've been a bit lazy in later lectures and really should\n",
    "# do this sort of thing more often to make code easier to read - apologies]\n",
    "def plot_raw_data(xi, yi, ax):\n",
    "    \"\"\"plot x vs y on axes ax, \n",
    "    add axes labels and turn on grid\n",
    "    \"\"\"\n",
    "    ax.plot(xi, yi, 'ko', label='raw data')\n",
    "    ax.set_xlabel('$x$', fontsize=16)\n",
    "    ax.set_ylabel('$y$', fontsize=16)\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# For clarity we are going to add a small margin to all the plots.\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# plot the raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Our simple raw data', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='upper left', fontsize=18);\n",
    "# loc='best' means we let matplotlib decide the best place for the\n",
    "# legend to go.  For other options see \n",
    "#  https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Simple interpolation example\n",
    "\n",
    "One of the simplest examples of interpolation is to simply fit a straight line between every two successive data points.\n",
    "\n",
    "This is termed **piecewise-linear** interpolation, and the resulting function is called a **piecewise-linear interpolant**.\n",
    "\n",
    "This is an example of the more general **piecewise-polynomial** interpolation - a piecewise quadratic discontinuous function was given in the example image above.\n",
    "\n",
    "[Note that we will return to piecewise polynomial interpolation a bit later.]\n",
    "\n",
    "Of course the default approach to plotting effectively performs piecewise-linear interpolation as we shall now see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# Plot a piecewise-linear approximation.\n",
    "# We get this simply by connecting the points with straight lines\n",
    "# and this is the default behaviour of the plotting routine so simply\n",
    "# involves a call to 'plot' with our data.\n",
    "ax1.plot(xi, yi, 'b', label='p/w linear interpolant')\n",
    "\n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=18)\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Raw data and its p/w linear interpolant', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial interpolation (revisited)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Recall that given a set of $N+1$ data points $(x_i, y_i)$ (with distinct $x_i$'s). \n",
    "\n",
    "We can use these $N+1$ pieces of distinct information to construct a polynomial with $N+1$ free parameters, i.e. a polynomial of degree $N$:\n",
    "\n",
    "$$ P_N(x) := \\alpha_0 + \\alpha_1 x + \\alpha_2 x^2 + \\alpha_3 x^3 + \\ldots \\alpha_N x^N, $$\n",
    "\n",
    "where $\\alpha_0, \\, \\alpha_1, \\, \\ldots, \\, \\alpha_N$ are the coefficients of our polynomial. \n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "The exact matching of the number of pieces of distinct data with the number of free parameters tells us that \n",
    "\n",
    "<br>\n",
    "\n",
    ">**The interpolating polynomial of the least degree is unique.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion over basis functions (e.g. monomials)\n",
    "\n",
    "Note that we can also interpret the polynomial interpolant above as a *linear combination* of a <a href=\"https://en.wikipedia.org/wiki/Basis_(linear_algebra)\">*basis*</a> made up of single-term polynomials: \n",
    "\n",
    "$$1, \\; x, \\; x^2, \\; \\ldots, \\; x^N.$$ \n",
    "\n",
    "These single term polynomials are also referred to as [*monomials*](https://en.wikipedia.org/wiki/Monomial). \n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "More generally we can write our interpolation function $f$ as\n",
    "\n",
    "$$f(x) = \\sum_{i=0}^N \\alpha_i \\phi_i(x) $$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\alpha_i$ for $i=0,\\,\\ldots, N$ are the finite number of *weights* we need to find. \n",
    "\n",
    "\n",
    "- $\\phi_i(x)$ for $i=0,\\,\\ldots, N$ are a finite number of prescribed *basis functions* \n",
    "\n",
    "<br>\n",
    "\n",
    "For the polynomial interpolation we are currently considering $\\phi_i(x) = x^i$ of course. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Recall the naive way of recovering the coefficients in polynomial interpolation ...\n",
    "\n",
    "in the case of three data points where we seek to find a quadratic function, substituting the data into the quadratic yields three linear equations which we can write as the linear system\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 \\\\\n",
    "1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\alpha_0\\\\\n",
    "\\alpha_1\\\\\n",
    "\\alpha_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "y_2\n",
    "\\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\text{or equivalently in matrix notation} \\;\\;\\;\\;\\;\\; V\\boldsymbol{\\alpha} =\\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "If we solve this system by inverting the matrix ($V$) we have our quadratic polynomial coefficients:  $\\boldsymbol{\\alpha} = V^{-1}\\boldsymbol{y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three data point example\n",
    "xi = [0.3, 0.5, 0.8]\n",
    "yi = [0.2, 0.8, 0.6]\n",
    "\n",
    "# use a function to construct the matrix above\n",
    "# note than numpy already has a function to do this\n",
    "V = np.vander(xi, increasing=True)\n",
    "\n",
    "\n",
    "print('V = \\n{}'.format(V))\n",
    "\n",
    "# use a numpy linear algebra solver to solve the system\n",
    "a = np.linalg.solve(V, yi)\n",
    "\n",
    "# output the coefficients for our quadratic we have computed\n",
    "print('\\n Our coefficients a = \\n{}\\n'.format(a))\n",
    "\n",
    "# show that they are the same as is obtained from \n",
    "# numpy's polyfit function (for a quadratic)\n",
    "# (which of course they should be, given we argued that this polynomial is unique)\n",
    "print('The output from np.polyfit(x, y, 2) = \\n {}'.format(np.polyfit(xi, yi, 2)))\n",
    "\n",
    "# Note that the order is reversed because numpy.poly* assumes decreasing\n",
    "# rather than the increasing powers of x which we have used above\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# plot the raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# x locations at which to evaluate and plot the quadratic polynomial\n",
    "x = np.linspace(0., 1., 100)\n",
    "\n",
    "# Set up a polynomial from the coefficients using numpy rather than writing out.\n",
    "# Use numpy.flip to reverse the coefficients as poly1d assume decreasing rather than\n",
    "# increasing powers - look at documentation\n",
    "p2 = np.poly1d(np.flip(a, 0))\n",
    "print('\\nWhich agrees with us as long as we reverse the order of our coefficients:')\n",
    "print('np.flip(a, 0) = \\n{}'.format(np.flip(a, 0)))\n",
    "\n",
    "# the p2 here is a function so evaluate it at our x locations\n",
    "y = p2(x)\n",
    "\n",
    "# and plot\n",
    "ax1.plot(x, y, 'b', label='Quadratic')\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Polynomial approx to three data points', fontsize=16)\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "# set bounds\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lagrange polynomial \n",
    "\n",
    "Recall that we asked the question previously - can we do better?\n",
    "\n",
    "Can we avoid the need to form and invert a linear system to find our coefficient?\n",
    "\n",
    "The answer was yes and the [Lagrange polynomial](http://mathworld.wolfram.com/LagrangeInterpolatingPolynomial.html) was a method to do this that required us to select a better set of basis functions.\n",
    "\n",
    "<br>\n",
    "\n",
    "Given a set of $(N+1)$ points as above, the Lagrange polynomial is defined as the linear combination\n",
    "\n",
    "$$L(x) := \\sum_{i=0}^{N} y_i \\ell_i(x),$$\n",
    "\n",
    "where the $\\ell_i(x)$ are a new choice for our basis functions known as the *Lagrange basis polynomials* and are defined by the product\n",
    "\n",
    "$$\\ell_i(x) := \\prod_{\\begin{smallmatrix}0\\le m\\le N\\\\ m\\neq i\\end{smallmatrix}} \\frac{x-x_m}{x_i-x_m} = \\frac{(x-x_0)}{(x_i-x_0)} \\cdots \\frac{(x-x_{i-1})}{(x_i-x_{i-1})} \\frac{(x-x_{i+1})}{(x_i-x_{i+1})} \\cdots \\frac{(x-x_N)}{(x_i-x_N)},$$\n",
    "\n",
    "where $0\\le i\\le N$,\n",
    "\n",
    "and the $y_i$ are the $N+1$ weights/coefficients corresponding to this basis.\n",
    "\n",
    "<br>\n",
    "\n",
    "Uniqueness of the interpolating polynomial of minimum degree means that the result from the use of monomials of Lagrange basis polynomials means that both approaches yield identical interpolating polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's use this polynomial\n",
    "\n",
    "$$f(x) = P_N(x) = 2 + 3x + 4x^2$$ \n",
    "\n",
    "to evaluate some $y$ values for three equally spaced $x$ values:\n",
    "\n",
    "we thus have the $(x,y)$ data pairs $(0,2)$, $(1,9)$, $(2,24)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The Lagrange polynomial can be immediately written down:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}L(x) & = 2 \\ell_0(x) + 9  \\ell_1(x) + 24 \\ell_2(x) \\\\[20pt]\n",
    "&= 2\\frac{(x-1)}{(0-1)}\\frac{(x-2)}{(0-2)} \n",
    "+ 9\\frac{(x-0)}{(1-0)}\\frac{(x-2)}{(1-2)} \n",
    "+ 24\\frac{(x-0)}{(2-0)}\\frac{(x-1)}{(2-1)}\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "and either by expanding and collecting terms, of visually, we can establish that this is identical to the original function.\n",
    "\n",
    "[We'll come back to this example and expansion a bit later]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n",
    "x = np.linspace(0,2,100)\n",
    "\n",
    "P = 2. + 3. * x + 4. * x**2\n",
    "\n",
    "L = 2. * (x-1)/(0-1) * (x-2)/(0-2) \\\n",
    "+ 9 * (x-0)/(1-0) * (x-2)/(1-2) \\\n",
    "+ 24 * (x-0)/(2-0) * (x-1)/(2-1)\n",
    "\n",
    "ax1.plot(x, P, 'b', label='$P_N(x)$')\n",
    "ax1.plot(x, L, 'r.', label='$L(x)$')\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('Compare two polynomial approximations', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using scipy.interpolate\n",
    "\n",
    "Recall that we can use [scipy.interpolate.lagrange](http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.lagrange.html)\n",
    "from Python's [SciPy](http://www.scipy.org) library to generate the Lagrange polynomial.\n",
    "\n",
    "First our three point example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our raw data from earlier - you can also test on our three data point example\n",
    "xi = np.array([0,1,2])\n",
    "yi = np.array([2,9,24])\n",
    "\n",
    "# Create the Lagrange polynomial for the given points.\n",
    "lp = si.lagrange(xi, yi)\n",
    "# recall above that we executed 'import scipy.interpolate as si'\n",
    "# and so this line is calling the 'lagrange' function from the \n",
    "# 'interpolate' sub-package within scipy.\n",
    "\n",
    "# Evaluate this function at a high resolution (100 points here) so that \n",
    "# we get a smooth well-resolved line when we plot our polynomial\n",
    "x = np.linspace(-0.5, 2.5, 100)\n",
    "\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# actually plot (x,y)=(x,lp(x)) on the axes with the label ax1\n",
    "ax1.plot(x, lp(x), 'b', label='Lagrange interpolating polynomial')\n",
    "\n",
    "# Overlay raw data on the same axes\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "ax1.set_title('Lagrange interpolating polynomial (SciPy)', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our larger data set from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Our raw data from earlier - you can also test on our three data point example\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# Create the Lagrange polynomial for the given points.\n",
    "lp = si.lagrange(xi, yi)\n",
    "# recall above that we executed 'import scipy.interpolate as si'\n",
    "# and so this line is calling the 'lagrange' function from the \n",
    "# 'interpolate' sub-package within scipy.\n",
    "\n",
    "# Evaluate this function at a high resolution (100 points here) so that \n",
    "# we get a smooth well-resolved line when we plot our polynomial\n",
    "x = np.linspace(0.4, 9.1, 100)\n",
    "\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# actually plot (x,y)=(x,lp(x)) on the axes with the label ax1\n",
    "ax1.plot(x, lp(x), 'b', label='Lagrange interpolating polynomial')\n",
    "\n",
    "# Overlay raw data on the same axes\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "ax1.set_title('Lagrange interpolating polynomial (SciPy)', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of the Lagrange basis functions and thus the Lagrange polynomial was a homework exercise when we considered interpolation in the Computational Mathematics module.\n",
    "\n",
    "It turns out that there is yet another way of writing the (unique) interpolating polynomial that results in an algorithm that is arguable easier to implement - this is termed the Newton polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Newton polynomial  [$\\star$]\n",
    "\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "Calculating the Newton polynomial (also called [Newton's divided difference interpolation polynomial](http://mathworld.wolfram.com/NewtonsDividedDifferenceInterpolationFormula.html)) yields the same polynomial as the Lagrange polynomial method (remember that the polynomial of minimum degree to pass through each data point is unique), but is arguably easier to implement (and this implementation is a useful coding exercise - see homework).\n",
    "\n",
    "<br>\n",
    "\n",
    "To derive this approach we write our degree $N$ polynomial in the following form\n",
    "\n",
    "\n",
    "$$ P_N(x) = a_0 +(x-x_0)a_1 + (x-x_0)(x-x_1)a_2 + \\cdots + \\left[(x-x_0)(x-x_1)\\ldots(x-x_{N-1})\\right]a_N,$$\n",
    "\n",
    "\n",
    "note that this is indeed a degree $N$ polynomial and as before was have $N+1$ free parameters  $a_0, a_1, \\ldots, a_N$, which we need to find using the $N+1$ pieces of information we have in the given data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Efficient derivation of an algorithm to compute the Newton polynomial follows from noticing that we can write this polynomial in a <a href=\"https://en.wikipedia.org/wiki/Recursion_(computer_science)\">*recursive form*</a>.  \n",
    "\n",
    "Consider for example a case with $N=3$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_3(x) &= a_0 +(x-x_0)a_1 + (x-x_0)(x-x_1)a_2 + (x-x_0)(x-x_1)(x-x_2)a_3\\\\[5pt]\n",
    "&= a_0 +(x-x_0)[a_1 + (x-x_1)[a_2 + (x-x_2)a_3]].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that substituting in the $x_i$ values leads to a set of simultaneous equations where we can easily evaluate the unknowns $a_0, a_1, \\ldots$ using what is termed 'back (or forward) substitution'.  \n",
    "\n",
    "We'll see an example of this now ...\n",
    "\n",
    "\n",
    "\n",
    "1. Substitute $x=x_0$: We have $a_0 = P_3(x_0)$, and we know that our interpolant $P_3(x)$ evaluated at $x_0$ must return $y_0$. Hence, \n",
    "\n",
    "<br>\n",
    "    \n",
    "$$a_0 = y_0.$$\n",
    "\n",
    "<br>\n",
    "\n",
    "    \n",
    "2. Now substitute $x=x_1$: We have $P_3(x_1) = a_0 +(x_1-x_0)a_1 = y_0 +(x_1-x_0)a_1 $, the LHS of this is $y_1$, and we know everything on the RHS as we have already calculated $a_0 = y_0$. We can thus trivially rearrange to yield\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ a_1 = \\frac{(y_1 - y_0)}{(x_1-x_0)}.$$\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Substituting $x=x_2$ yields \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& y_2 = P_3(x_2) = a_0 +(x_2-x_0)[a_1 + (x_2-x_1)a_2] = y_0 + (x_2-x_0)\\left[ \\frac{(y_1 - y_0)}{(x_1-x_0)} + (x_2-x_1)a_2\\right]\\\\[5pt]\n",
    "&\\implies a_2 = \\frac{ \\frac{(y_2 - y_0)}{(x_2-x_0)} - \\frac{(y_1 - y_0)}{(x_1-x_0)}}{x_2-x_1}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "4. And so on ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "To define an algorithm for this method in general let's first introducing the following [*divided difference*](https://en.wikipedia.org/wiki/Divided_differences) notation\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "\\Delta y_i &= \\frac{y_i-y_0}{x_i-x_0},\\;\\;    && i=1,2,\\ldots, N,\\\\[10pt]\n",
    "\\Delta^2 y_i &= \\frac{\\Delta y_i-\\Delta y_1}{x_i-x_1},\\;\\;    && i=2, 3,\\ldots, N,\\\\[10pt]\n",
    "&\\vdots\\\\[5pt]\n",
    "\\Delta^N y_N &= \\frac{\\Delta^{N-1} y_N-\\Delta^{N-1} y_{N-1}}{x_N-x_{N-1}}.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "With a bit of thought we can hopefully see from the above example that the coefficients of the interpolating polynomial in the general case are given by\n",
    "\n",
    "\n",
    "$$a_0=y_0,\\;\\;\\;\\;\\; a_1 = \\Delta y_1, \\;\\;\\;\\;\\; a_2 = \\Delta^2 y_2, \\;\\;\\;\\;\\; \\ldots \\;\\;\\;\\;\\; a_N = \\Delta^N y_N.$$\n",
    "\n",
    "\n",
    "See the homework exercise for an implementation of functions to construct and evaluate the Newton polynomial.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Approximating a function\n",
    "\n",
    "Rather than approximating/interpolating arbitrary discrete data given to us somehow (e.g. from observations, or from a very expensive computer code which has been run previously), we can of course use the same methods to approximate a given function. \n",
    "\n",
    "We may want to do this in order to approximate a complex/expensive function with a simpler, cheaper interpolating function.\n",
    "\n",
    "Our \"function\" may be a large expensive PDE solver for example.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that to avoid confusion I will use $f$ as the given function we wish to approximate and $f_h$ as the interpolating function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "For example, consider the function $y(x)=x^3$ with data provided at  equally spaced points.  \n",
    "\n",
    "Let's see what happens if we evaluate with Lagrange polynomials of degree 0, 1 and 2.\n",
    "\n",
    "[Note that since in this case we are given the underlying function, we can in principle choose the locations and number of data points, so for the degree 0, 1 and 2 Lagrange polynomials we first need to evaluate the underlying function 1, 2 and 3 times respectively. For simplicity below we just choose these to be evenly distributed in our $x$ domain of interest]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this is the function we are going to approximate with low degree polynomials\n",
    "def func_x3(x):\n",
    "    return x**3\n",
    "\n",
    "# as we will plot our approximation several times let's write a small function to do this\n",
    "def plot_approximation(f, xi, ax):\n",
    "    \"\"\"Function that plots an original function and its Lagrange polynomial approximation. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Original function\n",
    "    xi : array_like\n",
    "        The x-component of the data\n",
    "    ax : matplotlib axes\n",
    "        The axes to plot on\n",
    "    \"\"\"\n",
    "    # Relatively fine x points for plotting our functions\n",
    "    x = np.linspace(0.5, 3.5, 100)\n",
    "    # Plot the original function\n",
    "    ax.plot(x, f(x), 'k', label = 'Original function')\n",
    "\n",
    "    # construct and plot the Lagrange polynomial\n",
    "    lp = si.lagrange(xi, f(xi))\n",
    "    # evaluate and plot the Lagrange polynomial at the x points\n",
    "    ax.plot(x, lp(x), 'b', label = 'Lagrange poly. interpolant')\n",
    "\n",
    "    # shade the region between the two to emphasise the difference\n",
    "    ax.fill_between(x, f(x), lp(x))\n",
    "\n",
    "    # add some axis labels\n",
    "    ax.set_xlabel('$x$', fontsize=14)\n",
    "    ax.set_ylabel('$f(x), \\; P_N(x)$', fontsize=14)\n",
    "\n",
    "    # and add on top the interpolation points\n",
    "    ax.plot(xi, f(xi), 'ko')\n",
    "\n",
    "    # and a legend\n",
    "    ax.legend(loc='best', fontsize=13)\n",
    "\n",
    "\n",
    "# set up our figs for plotting - we want three subplots arranged in a 1x3 grid\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "# add some padding otherwise axes the labels can overlap with the next subplot\n",
    "fig.tight_layout(w_pad=4) \n",
    "\n",
    "# Plot the L0 degree 0 Lagrange poly and visualise the error\n",
    "plot_approximation(func_x3, np.array([2., ]), ax1)\n",
    "ax1.set_title('Approximating a cubic with a constant', fontsize=16)\n",
    "\n",
    "# Plot the L1 degree 1 Lagrange poly and visualise the error\n",
    "plot_approximation(func_x3, np.array([1., 3.]), ax2)\n",
    "ax2.set_title('Approximating a cubic with a linear', fontsize=16)\n",
    "\n",
    "# Plot the L2 degree 2 Lagrange poly and visualise the error\n",
    "plot_approximation(func_x3, np.array([1., 2., 3.]), ax3)\n",
    "ax3.set_title('Approximating a cubic with a quadratic', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error in Lagrange interpolation\n",
    "\n",
    "Note that it can be proven that in the case where we are interpolating a known function (e.g. a complicated non-polynomial function such as $\\exp$ or $\\sin$) by a simpler polynomial, the error at any point we evaluate the interpolant at is proportional to:\n",
    "\n",
    "\n",
    "- (1) the distance of that point from any of the data points (which makes sense as the error is obviously zero at the data points),\n",
    "\n",
    "\n",
    "- (2) and to the $(N+1)$-th derivative of that function evaluated at *some* location within the bounds of the data.  \n",
    "\n",
    "\n",
    "i.e. the more complicated (sharply varying) the function is, the higher the error *could* be.\n",
    "\n",
    "Note the similarities here with the Taylor series expansion.\n",
    "\n",
    "This result is sometimes called the [*Lagrange remainder theorem*](https://en.wikipedia.org/wiki/Polynomial_interpolation#Interpolation_error).\n",
    "\n",
    "We won't prove this here, but it will be useful in later lectures if we write it down now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Aside: The Lagrange Remainder Theorem  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "For $N\\ge 0$, let $f(x)$ be a function with at least $N+1$ continuous derivatives over the interval $[a,b]$, \n",
    "\n",
    "(the mathematical way of writing this is $f \\in C^{N+1}[a,b]$ - see [http://mathworld.wolfram.com/C-kFunction.html](http://mathworld.wolfram.com/C-kFunction.html) for more details on this if interested) \n",
    "\n",
    "and let $\\;\\;x_0<x_1<\\ldots<x_N\\;\\;$ be $N+1$ distinct points in $[a,b]$. \n",
    "\n",
    "Then the degree ($\\le$) $N$ polynomial interpolating $f(x)$ at these points, $P_N(x)$, satisfies\n",
    "\n",
    "\n",
    "$$ f(x) = P_N(x) + R_N(x) \\;\\;\\;\\;\\;\\; \\text{or} \\;\\;\\;\\;\\;\\; R_N(x) = f(x) - P_N(x) \\;\\;\\;\\;\\;\\; \\forall x\\in[a,b],$$\n",
    "\n",
    "\n",
    "where $R_N(x)$ is the *remainder*, or the [*interpolation error*](https://en.wikipedia.org/wiki/Polynomial_interpolation#Interpolation_error), and takes the form\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "R_N(x) & = \\frac{(x-x_0)(x-x_1)\\cdots(x-x_N)}{(N+1)!} \\, f^{(N+1)}(c_x) \\\\[10pt]\n",
    "& = \\Psi_N(x)\\,\\frac{f^{(N+1)}(c_x)}{(N+1)!},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $c_x$ is *some* value/point/location between the maximum and minimum values of $x_0, \\ldots, x_N$ and $x$,\n",
    "\n",
    "and where \n",
    "\n",
    "$$ \\Psi_N(x) := \\prod^N_{i=0} (x - x_i) = (x-x_0)(x-x_1)\\cdots(x-x_N). $$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "This gives us an expression for the error, but we don't know what $c_x$ is and so it's not that helpful.  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**However, we can use this result to derive an upper bound on the error:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "If you want you can think of the unknown $c_x$ as being some value $c_x \\in (a,b)$, and by setting \n",
    "\n",
    "$$M:=\\max_{x\\in(a,b)}\\, \\left| f^{(N+1)}(x) \\right|,$$ \n",
    "\n",
    "i.e. defining $M$ to be the *maximum* value of the ($N+1$)-th derivative over our interval, \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "then we are left with the error **bound** that no longer depends on this unknown $c_x$:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\left| f(x) - P_N(x) \\right| \\le \\frac{1}{(N+1)!}\\, M\\, \\left|\\Psi_N(x)\\right|,  \\\\[20pt]\n",
    "M:=\\max_{x\\in(a,b)}\\, \\left| f^{(N+1)}(x) \\right|, \\\\\n",
    "\\Psi_N(x) := \\prod^N_{i=0} (x - x_i) = (x-x_0)(x-x_1)\\cdots(x-x_N).\n",
    "}\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Observations  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "This looks complicated, but we can make multiple observations based on this quite easily\n",
    "(some which just reinforce things that are obvious, or that we have seen already):\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- (1) The error is zero at the nodes (as $\\Psi_N(x)$ vanishes at these points).\n",
    "\n",
    "<br>\n",
    "\n",
    "- (2) The error is zero everywhere if $f$ is a polynomial of degree $N$ or less (as then $f^{N+1}(x)\\equiv 0$), which is of course equivalent to saying that we can approximate a given polynomial function exactly if we match degrees.\n",
    "\n",
    "<br>\n",
    "\n",
    "- (3) This result is only true for smooth functions and essentially says that smooth functions behave like polynomials (cf. a truncated [Taylor series](https://en.wikipedia.org/wiki/Taylor_series) expansion) - this is why polynomial interpolation works for smooth functions.\n",
    "\n",
    "<br>\n",
    "\n",
    "- (4) However, this mathematical result (and hence all of our observations based on it) are invalid when $f$ has discontinuous derivatives (is not smooth) - because $f$ no longer behaves like a polynomial (e.g. it can do what it likes between the known points).\n",
    "\n",
    "<br>\n",
    "\n",
    "- (5) Functions with larger $(N+1)$-st derivative will tend to have larger interpolation errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "- (6) For a given function $f$, minimising the interpolation error for a given choice on $N$ is effectively the same as minimising $\\left|\\Psi_N(x)\\right|$ for $x\\in [a,b]$.  In addition, the structure of the error (i.e. how it varies in space) is due to the spatial structure of $\\Psi$ (as the other terms do not depend on $x$).\n",
    "\n",
    "<br>\n",
    "\n",
    "The final point is important - we will return to this shortly.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Example  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "What does this tell us for our example $f(x)=x^3$?\n",
    "\n",
    "As for our plots above with this example, let's assume $[a,b]=[0.5, 3.5]$ and compute the error (estimate or bound) based upon the above theory.\n",
    "\n",
    "We need the maximum value of the appropriate (i.e. dependent on our polynomial degree) degree derivative over the interval:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "N & = 0: \\;\\;\\;\\; & \\max_{x\\in(a,b)}f'(x) & = \\left. 3x^2 \\right|_{x = 3.5} = 36.75,\\\\[15pt]\n",
    "N & = 1: \\;\\;\\;\\; & \\max_{x\\in(a,b)}f''(x) & = \\left. 6x \\right|_{x = 3.5} = 21,\\\\[15pt]\n",
    "N & = 2: \\;\\;\\;\\; & \\max_{x\\in(a,b)}f'''(x) & =  6, \\\\[15pt]\n",
    "N & = 3: \\;\\;\\;\\; & \\max_{x\\in(a,b)}f^{(4)}(x) & = 0. \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's now evaluate the actual error between the function and the interpolating polynomials and see how well these error estimates agree, or bound this error.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def func_x3(x):\n",
    "    return x**3\n",
    "\n",
    "def plot_errors(f, xi, ax):   \n",
    "    N = len(xi) - 1\n",
    "\n",
    "    # M contains the maximum derivative values, as computed above;\n",
    "    # a component (the N-th) of which appear as one factor in our error estimate.\n",
    "    M = [36.75, 21., 6.]\n",
    "\n",
    "    # for the approximation with a Lagrange polynomial\n",
    "    lp = si.lagrange(xi, f(xi))\n",
    "\n",
    "    # plot the error between the function f(x) and the Lagrange polynomial over [0.5,3.5]\n",
    "    x = np.linspace(0.5, 3.5, 1000)\n",
    "    ax.plot(x, np.abs(f(x) - lp(x)), 'b', label = 'Error - actual')\n",
    "\n",
    "    # compute the Psi function defined above, here using \"list comprehension\"\n",
    "    Psi = np.prod([(x - xi[i]) for i in range(N+1)], axis=0)\n",
    "\n",
    "    # our error estimate is (1/(N+1)!) * M * |Psi|.  Plot this:\n",
    "    ax.plot(x, (1./factorial(N+1)) * M[N] * np.abs(Psi), 'r--', label = 'Error bound estimate')\n",
    "\n",
    "    # add labels and legend\n",
    "    ax.set_xlabel('$x$', fontsize=14)\n",
    "    ax.set_ylabel('Error', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=14)\n",
    "    \n",
    "# set up our figs for plotting - we want three subplots arranged in a 1x3 grid\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.tight_layout(w_pad=4)  # add some padding otherwise axes labels overlap\n",
    "\n",
    "# L0\n",
    "plot_errors(func_x3, np.array([2., ]), ax1)\n",
    "ax1.set_title('Theoretical vs exact error - constant', fontsize=16)\n",
    "\n",
    "# L1\n",
    "plot_errors(func_x3, np.array([1., 3.]), ax2)\n",
    "ax2.set_title('Theoretical vs exact error - linear', fontsize=16)\n",
    "\n",
    "# L2\n",
    "plot_errors(func_x3, np.array([1., 2., 3.]), ax3)\n",
    "ax3.set_title('Theoretical vs exact error - quadratic', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Comments  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "1. Our error bound is indeed an **upper bound** on the actual error.\n",
    "\n",
    "\n",
    "2. Both the actual error and the error bound are zero at the data locations. \n",
    "\n",
    "\n",
    "3. For this example (i.e. a 'simple' cubic function), our error bound is actually exactly equal to the real error in the case of the quadratic approximation - why is this?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "[When an error bound value is very close to the actual error we call it a *tight* bound - tighter bounds are obviously more useful in practice although not always necessary for proving theoretical results - <https://en.wikipedia.org/wiki/Upper_and_lower_bounds>].\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## A more problematic example [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "[This is an illustrative example to demonstrate how things can go wrong, especially when using high-degree approximating polynomials.]\n",
    "\n",
    "<br>\n",
    "\n",
    "Approximating the [Runge function](https://en.wikipedia.org/wiki/Runge%27s_phenomenon):\n",
    "\n",
    "$$f(x) := \\frac{1}{1 + 25 x^2},$$\n",
    "\n",
    "using equally spaced data points in the interval $[-1,1]$ is a famous example demonstrating how/where polynomial interpolation can go wrong!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the runge function\n",
    "def runge(x):\n",
    "    return 1.0 / (1.0 + 25.0 * x**2)\n",
    "\n",
    "def plot_approximation(f, xi, ax):\n",
    "    \"\"\"Function to plot exact function f (e.g. Runge) and its Lagrange polynomial approximation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The Runge function\n",
    "    xi : array_like\n",
    "        The x-component of the data\n",
    "    ax : matplotlib axes\n",
    "        The axes to plot on\n",
    "    \"\"\"\n",
    "    x = np.linspace(-1.0, 1.0, 100)\n",
    "    ax.plot(x, f(x), 'k', label='Runge function')\n",
    "\n",
    "    lp = si.lagrange(xi, f(xi))\n",
    "    ax.plot(x, lp(x), 'b', label='Poly. approx.')\n",
    "\n",
    "    ax.set_xlabel('$x$', fontsize=14)\n",
    "    ax.set_ylabel('$f(x)$', fontsize=14)\n",
    "\n",
    "\n",
    "# the problem gets worse at higher degree - list of degrees to consider\n",
    "degrees = [1, 2, 3, 5, 9, 12, 15, 20]\n",
    "\n",
    "# set up our figs for plotting\n",
    "fig, axs = plt.subplots(2, np.int(len(degrees)/2), figsize=(12, 8))\n",
    "# this turns the 2D array of axes into a 1D vector we can easily call in the loop below\n",
    "axs = axs.reshape(-1)\n",
    "# add some padding otherwise axes labels overlap\n",
    "fig.tight_layout(w_pad=3, h_pad=4)\n",
    "\n",
    "# enumerate is a useful way to loop over something (here degrees) and have \n",
    "# an automatic counter - here i which we use to identify the correct axis\n",
    "for i, degree in enumerate(degrees):\n",
    "    xi = np.linspace(-1, 1, degree + 1)\n",
    "    plot_approximation(runge, xi, axs[i])\n",
    "    axs[i].plot(xi, runge(xi), 'ko', label='data')\n",
    "    axs[i].set_title('Degree %i' % degree, fontsize=14)\n",
    "    axs[i].legend(loc='best', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Observations  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "\n",
    "- We see that for this problem, with increasing numbers of evenly spaced data points, the corresponding Lagrange polynomials, while doing a better and better job of approximating the underlying exact function *towards the middle* of the interval, do an increasingly bad job towards the *ends of the interval*.\n",
    "\n",
    "\n",
    "- **Take home message:**  Try to avoid high-degree interpolants when the data is evenly spaced - as high-degree polynomials have a tendency to oscillate strongly between data points, especially at the ends.\n",
    "\n",
    "\n",
    "- So interpolation here in regions of the domain would give huge errors. Imagine further what would happen if you tried to use one of these Lagrange polynomials to estimate a value for the Runge function slightly outside the $[-1,1]$ interval considered here (i.e. to **extrapolate**) - the result would clearly be terrible. This is therefore **also** a good example of why extrapolation in general should be undertaken with extreme caution!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### An explanation  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "The reason for this behaviour is a consequence of two factors:\n",
    "\n",
    "\n",
    "- Although smooth, the magnitude of the derivative of this function grows with $N$ [see homework exercise].\n",
    "\n",
    "\n",
    "- The behaviour of the $\\Psi$ quantity is such that, relatively speaking, with evenly spaced data points $\\Psi(x)$ is much larger in the outer sub-intervals than it is the middle of the domain - as we can see in the following plot.\n",
    "\n",
    "\n",
    "One of the homework exercises involves plotting the error bound - and the byproduct of multiplying things together which are very large/small is an error bound which we can indeed show gets very large with $N$ and with the error largest at the end points.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Psi for different N values:\n",
    "Ns = [0, 1,  5,  20]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "x = np.linspace(-1., 1., 1000)\n",
    "for N in Ns:\n",
    "    xi = np.linspace(-1, 1, N+1)\n",
    "    Psi = np.prod([(x - xii) for xii in xi],axis=0)\n",
    "    ax.plot(x, Psi / max( 1.e-10, np.max(np.abs(Psi)) ), label='N='+str(N) ) \n",
    "    ax.set_xlabel('$x$', fontsize=14)\n",
    "    ax.set_ylabel('$\\Psi(x)/\\max({\\Psi})$', fontsize=14)\n",
    "    ax.set_title('Relative size of $\\Psi$ across our interval', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=14)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "N = 20\n",
    "xi = np.linspace(-1, 1, N+1)\n",
    "Psi = np.prod([(x - xii) for xii in xi],axis=0)\n",
    "ax.plot(x, Psi , label='N='+str(N) ) \n",
    "ax.set_xlabel('$x$', fontsize=14)\n",
    "ax.set_ylabel('$\\Psi(x)$', fontsize=14)\n",
    "ax.set_title('Absolute size of $\\Psi$ across our interval', fontsize=14)\n",
    "ax.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chebyshev nodes  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "There were three components making up the error bound - the factorial of $N+1$, a bound on the $(N+1)$-st derivative of the function, and $\\Psi$. For a given function and polynomial degree, we have no control over the first two, but we do have some ability to control the third, and we saw above that the behaviour of $\\Psi$ is indeed a problem as $N$ grows.\n",
    "\n",
    "The above example can be \"fixed\" (or at least we can attempt to do a better job) by seeking to minimise $\\Psi(x)$ over our entire interval (including the outer sub-intervals where above and in the homework we see there's a clear issue).\n",
    "\n",
    "For a fixed $N$, the only thing we have the ability to change is to choose different $x_i$ data points, i.e. to sample our function at non-uniformly spaced *locations*.\n",
    "\n",
    "Note that by the definition/construction of $\\Psi$, these *locations* are the *roots* of $\\Psi$, i.e. the locations where $\\Psi$ is zero.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "So said another way, we want to find/select the polynomial $\\Psi$ that is smallest over our interval; the optimal sampling data points for our interpolation process will then be the roots of this polynomial. \n",
    "\n",
    "It turns out that \n",
    "<a href=\"https://en.wikipedia.org/wiki/Chebyshev_polynomials#Minimal_%E2%88%9E-norm\">*Chebyshev polynomials*</a> have this minimal property, and hence our optimal sample points are the roots of these Chebyshev polynomials - since they are so important these points have their own name: [*Chebyshev nodes*](https://en.wikipedia.org/wiki/Chebyshev_nodes).\n",
    "\n",
    "The Chebyshev nodes in the interval $(-1,1)$ are given by the formula\n",
    "\n",
    "$$x_i = \\cos \\left (\\frac{2i - 1}{2N}\\pi \\right ), \\;\\;\\;\\; i = 1, \\ldots, N,$$\n",
    "\n",
    "and as we will see in the following example, evaluating our function at these non-uniform locations and using this data to construct our interpolating polynomial fixes the problem we saw above for the case where we used evenly spaced data.\n",
    "\n",
    "In the next cell we again compute and plot the Lagrange polynomial interpolant for differing $N$, but simply change the location of the data sampling points `xi`. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need to consider quite so many cases now to prove the point that\n",
    "# things have improved substantially.\n",
    "degrees = [5, 9, 12, 20]\n",
    "\n",
    "# set up our figs for plotting\n",
    "fig, ax = plt.subplots(1, len(degrees), figsize=(12, 4))\n",
    "fig.tight_layout(w_pad=2)  # add some padding otherwise axes labels overlap\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # use the Chebyshev nodes instead\n",
    "    xi = np.cos((2.0 * np.arange(1, degree+2) - 1.0) * np.pi / (2.0 * (degree+1)))\n",
    "    # compute and plot the Lagrange polynomial using Chebyshev nodes as data locations\n",
    "    plot_approximation(runge, xi, ax[i]) # recall that this function also computes L_p\n",
    "    ax[i].plot(xi, runge(xi), 'ko', label='data')\n",
    "    ax[i].set_title('Degree %i' % degree, fontsize=16)\n",
    "    ax[i].legend(loc='best', fontsize=14)\n",
    "\n",
    "# add another figure to plot Chebyshev node locations\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 1))\n",
    "ax.plot(xi, np.zeros_like(xi), 'ko')\n",
    "ax.yaxis.set_ticks([])\n",
    "ax.set_xlabel('$x$', fontsize=16)\n",
    "ax.set_title('Locations of the Chebyshev nodes in the degree 20 case', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We see that rather than being evenly distribute the data points are now clustered towards the extremes of the domain.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "## Chebyshev polynomial interpolation  [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "This in turn leads us to a new set of basis functions which we could use as the basis for our polynomial interpolation.\n",
    "\n",
    "In the same way that the values of the underlying function at our data point locations were, by construction, the weights for the Lagrange basis functions we defined above, the values of the underlying function at the non-uniform Chebyshev nodes are the weights for an interpolant which uses Chebyshev polynomials as basis functions [see homework exercise].\n",
    "\n",
    "Summing, the resulting function is called the *Chebyshev polynomial interpolant*.\n",
    "\n",
    "Chebyshev nodes/interpolation have many nice theoretical properties, which we won't cover here.\n",
    "\n",
    "Note that the construction of these basis functions is a homework exercise, where the exact details are explained in the question.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise polynomial interpolation \n",
    "\n",
    "An alternative approach to overcome the issues identified above which attempted to fit a *single* *high-degree* polynomial to multiple data points, is to split the data into pieces and fit a lower degree polynomial through each of these.\n",
    "\n",
    "The piecewise (p/w) linear interpolation we saw near the start of this lecture would of course be an example of this.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise-linear interpolant\n",
    "\n",
    "The implementation of piecewise (p/w) linear interpolation would simply take two data points $(x_i,y_i)$ and $(x_{i+1},y_{i+1})$ in turn and fit a linear polynomial between them:\n",
    "\n",
    "\n",
    "$$P_1^i(x) = \\frac{ y_{i+1} - y_i }{ x_{i+1} - x_i }(x - x_i) + y_i,$$\n",
    "\n",
    "\n",
    "the superscript $i$ is included to emphasise here that we are only considering one (the $i$-th) sub-interval.\n",
    "\n",
    "This is easy to implement ourselves using this formula; we could reuse our Lagrange interpolation code, or indeed as we saw above simply plotting the data we get this result as well.\n",
    "\n",
    "Alternatively we could just use the `numpy.polyfit` function to fit the appropriate degree polynomial to the data; we will see more on this below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise-quadratic interpolant\n",
    "\n",
    "We can extend the linear example above by fitting a quadratic to three data points.\n",
    "\n",
    "So for every 3 sets of points we fit a quadratic - to keep things simple we therefore require $3+2\\times n$ data points, for some integer $n$ (i.e. the first set of three, and then sets of two extra as we reuse the last entry from the previous set of three as the first of the new set).  We therefore need to add a point to our example data set from above to satisfy this constraint.\n",
    "\n",
    "This time we will make use of `numpy.polyfit` to construct each quadratic for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra point to our data so that we have three complete sets of data\n",
    "xi = np.array([0.5, 2.0, 3.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.5, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# construct the interpolant at these x points for plotting\n",
    "x = np.linspace(np.min(xi), np.max(xi), 1000)\n",
    "\n",
    "# initialise the value of our interpolant at x to zero\n",
    "P2 = np.zeros_like(x)\n",
    "\n",
    "# loop over collections of three data points from (xi,yi), i is the mid value\n",
    "for i in range(1, len(xi), 2):\n",
    "    # use polyfit to construct a local quadratic polynomial fit to data at i-1, i, i+1\n",
    "    P2_loc = np.polyfit(xi[i - 1:i + 2], yi[i - 1:i + 2], 2)\n",
    "    # use polyval to evaluate P2_loc at the x values for x in [x_{i-1},x_{i+1}]\n",
    "    P2 += np.polyval(P2_loc, x) * (x >= xi[i - 1]) * (x <= xi[i + 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "ax1.plot(xi, yi, 'b', label='p/w linear')\n",
    "ax1.plot(x, P2, 'r', label='p/w quadratic')\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "ax1.set_title('p/w linear and p/w quadratic approximations', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Observations\n",
    "\n",
    "\n",
    "- An obvious drawback of this we see from both the p/w linear and p/w quadratic examples above is that we lose *smoothness* - the interpolant now has jumps in its derivative (sharp corners).\n",
    "\n",
    "\n",
    "- Since the degree two polynomial that fits through three data points is unique, we could also of course have used our Lagrange polynomial code rather than polyfit in the implementation above (ditto the linear case fit to two points). We used polyfit just to demonstrate/practice using other standard Python (NumPy) functions.\n",
    "\n",
    "\n",
    "- Note that there are p/w interpolating options which preserve smoothness to varying degrees; these include [cubic splines](https://en.wikipedia.org/wiki/Spline_interpolation) and [cubic Hermite polynomials](https://en.wikipedia.org/wiki/Cubic_Hermite_spline) which we will introduce briefly next.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise-polynomial interpolation via an appropriate set of basis functions\n",
    "\n",
    "\n",
    "### The idea\n",
    "\n",
    "Recall that in terms of basis functions ($\\phi_i(x)$) we saw above how we can write a polynomial in several different ways.\n",
    "\n",
    "Can we find an appropriate basis that allows us to represent a piece-wise polynomial?\n",
    "\n",
    "Note that this is a vital step in the finite element method which we shall return to in a later lecture.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's consider the expansion of our interpolating function from earlier:\n",
    "\n",
    "$$f(x) = \\sum_{i=0}^N \\alpha_i \\phi_i(x), $$\n",
    "\n",
    "what are the basis functions $\\phi_i(x)$ that recreate the piecewise linear interpolant?\n",
    "\n",
    "<br>\n",
    "\n",
    "A key observation is that with piecewise polynomials, linear say, it's only the local data values that determine the behaviour of the interpolant in between the data points. \n",
    "\n",
    "<br>\n",
    "\n",
    "Note that with monomials, Lagrange basis functions etc, they are defined and have values at all $x$ values, and hence a single data point value and its corresponding basis function influences the interpolant everywhere \n",
    "\n",
    "(although as we will see shortly, we can if we wish specify our basis functions to be zero almost everywhere, in which case this observation is no longer true).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - revisiting a quadratic example we've already seen\n",
    "\n",
    "E.g. for our earlier example quadratic function $P(x)$ we can consider each of the three monomial and Lagrange basis polynomials individually over the interval covered by three data points and demonstrate that with the correctly chosen weights we recover $p$.\n",
    "\n",
    "For completeness, we also demonstrate this for the Newton polynomial expansion ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's demonstrate how to build up our desired quadratic poly using\n",
    "# monomial basis functions.\n",
    "# The weights here are given by the known polynomial coefficients, i.e. 2, 3, 4\n",
    "# while the basis functions are the constant (1), the linear (x) and the quadratic (x**2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "fig.tight_layout(w_pad=2)  \n",
    "\n",
    "x = np.linspace(0,2,100)\n",
    "\n",
    "P = 2. + 3. * x + 4. * x**2\n",
    "\n",
    "p0 =  np.ones_like(x)\n",
    "p1 =  x\n",
    "p2 =  x**2\n",
    "\n",
    "ax[0].plot(x, p0, 'r'); ax[0].set_title('$p_0=1$', fontsize=14)\n",
    "ax[1].plot(x, p1, 'r'); ax[1].set_title('$p_1=x$', fontsize=14)\n",
    "ax[2].plot(x, p2, 'r'); ax[2].set_title('$p_2=x^2$', fontsize=14)\n",
    "ax[3].plot(x, 2*p0 + 3*p1 + 4*p2, 'r.', label=r'$\\sum \\alpha_i p_i(x)$')\n",
    "ax[3].set_title(r'P vs $\\sum \\alpha_i p_i(x)$', fontsize=14)\n",
    "ax[3].plot(x, P, 'b', label='$P(x)$')\n",
    "ax[3].legend(loc='best', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's use an expansion over the Lagrange basis functions\n",
    "# We know that the Lagrange basis functions are chosen specially\n",
    "# so that the corresponding weights are the data points, i.e. here\n",
    "# alpha_i = P(x_i), with x_i = 0, 1, and 2 - this gives weights\n",
    "# 2, 9 and 24.\n",
    "# The basis functions are this time each quadratics - \n",
    "# but they special quadratics # that are one at one x location, and zero at the other two:\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "fig.tight_layout(w_pad=2)  \n",
    "\n",
    "x = np.linspace(0,2,100)\n",
    "\n",
    "P = 2. + 3. * x + 4. * x**2\n",
    "\n",
    "l0 =  (x-1)/(0-1) * (x-2)/(0-2) \n",
    "l1 =  (x-0)/(1-0) * (x-2)/(1-2) \n",
    "l2 =  (x-0)/(2-0) * (x-1)/(2-1)\n",
    "\n",
    "ax[0].plot(x, l0, 'r'); ax[0].set_title('$\\ell_0(x)$', fontsize=14)\n",
    "ax[0].plot([0,2], [0,0], 'b--')\n",
    "ax[1].plot(x, l1, 'r'); ax[1].set_title('$\\ell_1(x)$', fontsize=14)\n",
    "ax[1].plot([0,2], [0,0], 'b--')\n",
    "ax[2].plot(x, l2, 'r'); ax[2].set_title('$\\ell_2(x)$', fontsize=14)\n",
    "ax[2].plot([0,2], [0,0], 'b--')\n",
    "ax[3].plot(x, 2*l0 + 9*l1 + 24*l2, 'r.', label='$L(x)$'); ax[3].set_title('P vs L', fontsize=14)\n",
    "ax[3].plot(x, P, 'b', label='$P(x)$')\n",
    "ax[3].legend(loc='best', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in the case of the Newton polynomial, for completeness\n",
    "# The basis functions are really just shifted versions of the monomial case,\n",
    "# and the weights are then known from the above formula\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "fig.tight_layout(w_pad=2)  \n",
    "\n",
    "x = np.linspace(0,2,100)\n",
    "\n",
    "P = 2. + 3. * x + 4. * x**2\n",
    "\n",
    "n0 =  np.ones_like(x) \n",
    "n1 =  (x-0)\n",
    "n2 =  (x-0)*(x-1)\n",
    "\n",
    "a0 = 2 # y0\n",
    "a1 = (9-2)/(1-0) # (y1-y0)/(x1-x0)\n",
    "a2 = ((24-2)/(2-0) - (9-2)/(1-0)) / (2-1) # ((y2-y0)/(x2-x0) - (y1-y0)/(x1-x0)) / (x2-x1)\n",
    "\n",
    "ax[0].plot(x, n0, 'r'); ax[0].set_title('$n_0(x)$', fontsize=14)\n",
    "ax[1].plot(x, n1, 'r'); ax[1].set_title('$n_1(x)$', fontsize=14)\n",
    "ax[2].plot(x, n2, 'r'); ax[2].set_title('$n_2(x)$', fontsize=14)\n",
    "ax[3].plot(x, a0*n0 + a1*n1 + a2*n2, 'r.', label='$N(x)$'); ax[3].set_title('P vs N', fontsize=14)\n",
    "ax[3].plot(x, P, 'b', label='$P(x)$')\n",
    "ax[3].legend(loc='best', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hat functions and p/w linear interpolation/expansions\n",
    "\n",
    "For p/w linear interpolants with a bit of thought we can come up with some appropriate basis functions, which only contribute to the full interpolant locally.\n",
    "\n",
    "<br>\n",
    "\n",
    "These are\n",
    "\n",
    "$$\n",
    "\\phi_i(x) = \n",
    "\\begin{cases}\n",
    "\\frac{x - x_{i-1}}{\\Delta x_{i-1}}, &\\text{if}\\;\\;\\; x\\in e_{i-1}\\equiv[x_{i-1},x_{i}] \\\\[5pt]\n",
    "\\frac{x_{i+1} - x}{\\Delta x_{i}}, &\\text{if}\\;\\;\\; x\\in e_{i}\\equiv[x_{i},x_{i+1}] \\\\[5pt]\n",
    "0, & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "which have the name hat (or chapeau) functions, and look like the following.\n",
    "\n",
    "```{figure} Figures/fem_basis_functions.svg\n",
    ":width: 75%\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code to return the hat/basis functions\n",
    "\n",
    "Let's implement a function that returns the $i$-th hat function on a mesh.  You can try playing with the value for $i$ and seeing how it changes the plot - this code should work find if you don't have uniform x spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat_function(i, x_nodes):\n",
    "    \"\"\" Linear continuous basis function at node i.\n",
    "    Returns a function for later use in quadrature rules.\n",
    "    \"\"\"\n",
    "    assert(i>=0 and i<=len(x_nodes)-1)\n",
    "    dx = np.diff(x_nodes)\n",
    "    if i==0:\n",
    "        hat = lambda x: np.select([ x<=x_nodes[i+1] ], [ (x_nodes[i+1] - x)/dx[i] ] ,0.0)\n",
    "    elif i==len(x_nodes)-1:\n",
    "        hat = lambda x: np.select([ x>=x_nodes[i-1] ], [ (x - x_nodes[i-1])/dx[i-1] ] ,0.0)\n",
    "    else:\n",
    "        hat = lambda x: np.select(\n",
    "                [ np.all([[x>=x_nodes[i-1]],[x<=x_nodes[i]]], axis=0)[0],  \n",
    "                  np.all([[x>=x_nodes[i]],[x<=x_nodes[i+1]]], axis=0)[0]] ,\n",
    "                [ (x-x_nodes[i-1])/dx[i-1], (x_nodes[i+1]-x)/dx[i] ] ,0.0)\n",
    "    return hat\n",
    "\n",
    "# let's plot what this function returns\n",
    "# first set up the nodes, a fine mesh to use to plot and the axes\n",
    "x_nodes = np.linspace(0,1,8)\n",
    "x_fine = np.linspace(0,1,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks((x_nodes))\n",
    "ax1.set_xticklabels(('$x_0$','$x_1$','$x_2$','$x_3$','$x_4$','$x_5$','$x_6$','$x_7$' ), fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('P1 basis function', fontsize=16)\n",
    "\n",
    "# you can change the following to be in [0,n] to change what the \n",
    "# function returns for a given node number\n",
    "node = 2\n",
    "phi = hat_function(node, x_nodes)\n",
    "# it's returned a function phi, so just evaluate this on the mesh\n",
    "ax1.plot(x_fine, phi(x_fine), 'k-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that this does what we want in terms of delivering p/w linear interpolation let's add two of these basis functions\n",
    "\n",
    "\n",
    "Let's assume our $(x,y)$ data is $(x_3,4)$ and $(x_4,7)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot what this function returns\n",
    "# first set up the nodes, a fine mesh to use to plot and the axes\n",
    "x_nodes = np.linspace(0,1,8)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks((x_nodes))\n",
    "ax1.set_xticklabels(('$x_0$','$x_1$','$x_2$','$x_3$','$x_4$','$x_5$','$x_6$','$x_7$' ), fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('P1 basis function', fontsize=16)\n",
    "\n",
    "phi_l = hat_function(3, x_nodes)\n",
    "phi_r = hat_function(4, x_nodes)\n",
    "\n",
    "# plot on a fine mesh, only between the considered $x$ locations [3,4]\n",
    "x_fine = np.linspace(x_nodes[3],x_nodes[4],1000)\n",
    "\n",
    "ax1.plot(x_fine, 4*phi_l(x_fine) + 7*phi_r(x_fine), 'k-')\n",
    "ax1.plot([x_nodes[3],x_nodes[4]],[4,7], 'ro');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that when only considered between the data point locations, we indeed have the correct linear - what does this interpolant do outside these data points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our basis functions to construct the piecewise linear function $f_h(x)$, where the required weights are just given by the function $f$ evaluates at the node locations (`y_nodes = f(x_nodes)`), for a more complex $f$ function.\n",
    "\n",
    "In this implementation our function returns the values of $f_h(x)$ at a prescribed array of $x$ locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from an arbitrary function we wish to approximate/interpolate\n",
    "def f(x):\n",
    "    return np.sin(2*np.pi*x) + 0.3*np.cos(3.*2.*np.pi*x**2)\n",
    "\n",
    "# discrete data\n",
    "N_nodes = 4\n",
    "# if we have 4 nodes, including at the ends of our domain, then we have 4-1=3 elements/cells\n",
    "N_elements = N_nodes - 1\n",
    "# and the element size is the total interval length divided by the number of elements\n",
    "dx = 1./N_elements\n",
    "# construct the node locations\n",
    "x_nodes = np.linspace(0., 1., N_nodes)\n",
    "\n",
    "# and evaluate our function at these points - these will be our weights\n",
    "y_nodes = f(x_nodes)\n",
    "\n",
    "# now a function to reconstruct the p/w linear function\n",
    "def pwlinear(x_nodes, y_nodes, x):\n",
    "    \"\"\"Function to return f_h(x) at locations given in x, \n",
    "    with top hat functions on a mesh given by x_nodes, and\n",
    "    corresponding weights given in y_nodes.\n",
    "    \"\"\"\n",
    "    val = np.zeros_like(x)\n",
    "    # loop over x values, equivalently the basis functions\n",
    "    for i in range(len(x)):\n",
    "        # which mesh node is directly to the left of x[i] ?\n",
    "        node = np.where( x[i] >= x_nodes )[0][-1]\n",
    "        # because of the greater than or equals above, deal with problem of hitting last node\n",
    "        node = np.minimum(len(x_nodes)-2,node)\n",
    "        # add together the contributions from the two hat functions at this x location - \n",
    "        # the hat functions centred on \"node\" (to the left)\n",
    "        phi_node = hat_function(node, x_nodes)\n",
    "        # and \"node+1\" (to the right)\n",
    "        phi_node_plus_1 = hat_function(node+1, x_nodes)\n",
    "        # now take the weighted sum of these two hat functions\n",
    "        val[i] = y_nodes[node]*phi_node(x[i]) + y_nodes[node+1]*phi_node_plus_1(x[i])\n",
    "    return val\n",
    "\n",
    "x_fine = np.linspace(0,1,100)\n",
    "f_h = pwlinear(x_nodes, y_nodes, x_fine)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_xticks((x_nodes[0],x_nodes[1],x_nodes[2],x_nodes[3]))\n",
    "ax1.set_xticklabels(('$x_0$','$x_1$','$x_2$','$x_3$', ), fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('Interpolant reconstructed using hat functions', fontsize=16)\n",
    "ax1.plot(x_fine, f(x_fine), 'k-', label = 'exact')\n",
    "ax1.plot(x_fine, f_h, 'b-', label = 'p/w linear using hat functions')\n",
    "ax1.plot(x_nodes, f(x_nodes), 'bo')\n",
    "ax1.legend(loc='best', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as expected the expansion of these hat (basis) functions goes through the values given by the weights, with straight lines in between.\n",
    "\n",
    "We can expand this to p/w higher order polynomial interpolants, e.g. by using appropriate quadratic functions only defined as being non-zero in between appropriate data locations [cf. the quadratic example above]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support of basis functions\n",
    "\n",
    "An important aspect of the finite element method (which we'll go over properly in later lecture) is appreciating when the basis functions and in particular the products of basis functions, e.g.\n",
    "\n",
    "$$\\phi_i \\, \\phi_j\\,,$$\n",
    "\n",
    "are non-zero.\n",
    "\n",
    "We call the area a function is non-zero its *support*, and with the FEM we are looking for this area to be finite and for each basis function for this to be confined to a small portion of the overall domain - the mathematical term is [*compact support*](https://mathworld.wolfram.com/CompactSupport.html).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "For a given value of $\\,i$, based upon the above schematic for what values of $\\,j\\,$ is the product non-zero anywhere (alternatively, for what choices of $j$, given $i$, is the product zero everywhere?).\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Similarly note that the derivative of a basis function $\\,\\phi_{i,x}\\,$, has the same (finite) support as $\\,\\phi_i\\,$ and hence the same compact support result holds for the products\n",
    "\n",
    "$$\\phi_{i}\\,\\phi_{j,x}\\,, \\;\\;\\;\\;\\; \\phi'_{i}\\,\\phi_{j}\\,,\\;\\;\\;\\;\\;\\text{and}\\;\\;\\;\\; \\phi'_{i}\\,\\phi'_{j}\\,,$$\n",
    "\n",
    "where the subscript \"comma $x$\" and the primes in the line above are just notation to indicate the $x$ derivatives of the basis functions.\n",
    "\n",
    "<br>\n",
    "\n",
    "To appreciate this, first you should think about what the derivatives of these hat functions look like - this is a homework exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cubic splines [$\\star\\star$]\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "This approach interpolates data with a piecewise cubic polynomial, see\n",
    "[https://en.wikipedia.org/wiki/Spline_interpolation](https://en.wikipedia.org/wiki/Spline_interpolation).\n",
    "\n",
    "Note that as opposed to the p/w quadratic example above, a cubic spline assumes a cubic polynomial **within** each interval between two data points.\n",
    "\n",
    "Due to the use of a cubic polynomial within each interval, this approach can now be made to be twice continuously differentiable (i.e. first as well as second order derivatives are continuous).\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Algorithm sketch  [$\\star\\star$]\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "The idea behind cubic splines comes from something physical: \n",
    "\n",
    "Pass a thin beam or elastic ruler between the data points (often termed *knots* when talking about splines), the shape you get is a cubic spline.  \n",
    "\n",
    "The term spline comes form the name of the [thin flexible strip](https://en.wikipedia.org/wiki/Flat_spline) used by draftsmen for engineering design before computers.\n",
    "\n",
    "The deflection of the beam can be computed by solving a [fourth-degree differential equation](https://en.wikipedia.org/wiki/Euler%E2%80%93Bernoulli_beam_theory).  Since we assume that no force is applied between the knots (data points), this *fourth*-degree equation is equal to zero between knots and hence the solution is a *cubic* between them. Over the entire range of points the deflection is therefore piecewise cubic.  Hence the name *cubic spline*!\n",
    "\n",
    "The first and second derivatives are continuous across the knots.\n",
    "\n",
    "Given $N+1$ data points, i.e. $N$ intervals, we have a cubic on each interval. Each cubic has four free parameters. To uniquely define all of these parameters we therefore need $4N$ pieces of information. The cubic passing through the data at the two ends of each interval gives $2N$ pieces of information. Continuity of the first derivative between each interval gives $N-1$ pieces of information, and continuity of the second derivative between each interval gives another $N-1$ pieces of information. [Note that we assume we don't know the exact derivative (of $f$), and hence the actual value of the derivatives of the spline are not constructed to match those of $f$].  \n",
    "\n",
    "We are therefore missing two pieces of information and we have several choices for what to do here. \n",
    "\n",
    "*Natural* cubic splines set the second derivative to zero at the start of the first interval and at the end of the last interval. This option is called *natural* since it is what would occur naturally in the beam analogy. \n",
    "See `bc_type` under the [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html) for more options.\n",
    "\n",
    "Due to the coupling between intervals a linear system needs to be solved (as written here: <http://mathworld.wolfram.com/CubicSpline.html>) to compute the $4N$ parameters - this is generally in the form of a tridiagonal system. We will cover the solution of linear systems such as these in L3.\n",
    "\n",
    "Rather than implementing ourselves we will make use of the SciPy function [scipy.interpolate.CubicSpline](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.interpolate.CubicSpline.html#scipy.interpolate.CubicSpline).\n",
    "\n",
    "\n",
    "[If you're feeling brave there is a **homework exercise** asking you to implement cubic spline interpolation.].\n",
    "\n",
    "\n",
    "Let's see what it does for our data set case, as well as the problematic Runge function in the two cells below - we see that it performs well.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the above example data again\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# use SciPy to compute the cubic spline\n",
    "Pcs = si.CubicSpline(xi, yi)\n",
    "\n",
    "# Note we would get the same result with\n",
    "# Pcs = si.interp1d(xi, yi, 'cubic', fill_value='extrapolate')\n",
    "# and with\n",
    "# Pcs = si.UnivariateSpline(xi, yi, s=0, k=3)\n",
    "# where s is a smoothing parameter - s>0 means we won't necessarily go through data exactly\n",
    "# and k is the spline degree - 3 = cubic, 1 and we are back to p/w linear\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "x = np.linspace(0.4, 9.1, 100)\n",
    "\n",
    "# Scipy actually formed a function Pcs for us that we need to evaluate at x to plot\n",
    "ax1.plot(x, Pcs(x), 'b', label='Cubic Spline')\n",
    "\n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('Cubic spline approximation to our data', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test on the Runge function which caused problems from Lagrange interpolation\n",
    "def runge(x):\n",
    "    return 1.0 / (1.0 + 25.0 * x**2)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "x = np.linspace(-1.0, 1.0, 100)\n",
    "ax1.plot(x, runge(x), 'k', label='exact')\n",
    "\n",
    "xi = np.linspace(-1, 1, 8)\n",
    "Pcs = si.CubicSpline(xi, runge(xi))\n",
    "ax1.plot(x, Pcs(x), 'b', label='Cubic spline')\n",
    "\n",
    "ax1.plot(xi, runge(xi), 'ro', label='data')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('Cubic spline approximation to Runge function', fontsize=16)\n",
    "\n",
    "# add the second subplot\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "ax1.plot(x, runge(x), 'k', label='exact')\n",
    "\n",
    "xi = np.linspace(-1, 1, 18)\n",
    "Pcs = si.CubicSpline(xi, runge(xi))\n",
    "ax1.plot(x, Pcs(x), 'b', label='Cubic spline')\n",
    "\n",
    "ax1.plot(xi, runge(xi), 'ro', label='data')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('Cubic spline approximation to Runge function', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Piecewise Cubic Hermite Interpolating Polynomial (PCHIP)  [$\\star\\star$]\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "If we drop the requirement for continuity over second derivatives with the cubic spline above (and hence the conditions that were used to fix all the free parameters in the cubic) we can choose the remaining free parameters to achieve a different goal ...\n",
    "\n",
    "If the (continuous) slopes between intervals are chosen in order for the interpolant to **preserve monotonicity** of the data then we have what is termed a [*p/w cubic Hermite interpolating polynomial*](https://uk.mathworks.com/help/matlab/ref/pchip.html) or a [*cubic Hermite spline*](https://en.wikipedia.org/wiki/Cubic_Hermite_spline).\n",
    "\n",
    "*Monotonicity* here means that the interpolant remains within the bounds of the $y$-data - [https://en.wikipedia.org/wiki/Monotone_cubic_interpolation](https://en.wikipedia.org/wiki/Monotone_cubic_interpolation)\n",
    "\n",
    "\n",
    "We will make use of [scipy.interpolate.PchipInterpolator](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.interpolate.PchipInterpolator.html#scipy.interpolate.PchipInterpolator)\n",
    "\n",
    "\n",
    "Some properties:\n",
    "\n",
    "\n",
    "- The interpolator preserves monotonicity in the interpolation data and does not overshoot if the data is not smooth.\n",
    "\n",
    "\n",
    "- The first derivatives are guaranteed to be continuous, but the second derivatives may jump at the $x_i$ locations. So while being monotonic/bounded, the result is a bit less smooth than for cubic splines.\n",
    "\n",
    "\n",
    "Let's see what it does for our data set case, and the problematic Runge function.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the above example data again\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "Pchip = si.PchipInterpolator(xi, yi)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "x = np.linspace(0.4, 9.1, 100)\n",
    "\n",
    "ax1.plot(x, Pchip(x), 'b', label='PCHIP')\n",
    "\n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('PCHIP approximation to data, demonstrating monotonicity w.r.t. data', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test on the Runge function which caused problems from Lagrange interpolation\n",
    "def runge(x):\n",
    "    return 1.0 / (1.0 + 25.0 * x**2)\n",
    "\n",
    "\n",
    "x = np.linspace(-1.0, 1.0, 100)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(x, runge(x), 'k', label='exact')\n",
    "ax1.margins(0.1)\n",
    "\n",
    "xi = np.linspace(-1, 1, 8)\n",
    "Pchip = si.PchipInterpolator(xi, runge(xi))\n",
    "ax1.plot(x, Pchip(x), 'b', label='PCHIP')\n",
    "\n",
    "ax1.plot(xi, runge(xi), 'ro', label='data')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=16)\n",
    "ax1.set_title('PCHIP approximation to Runge function', fontsize=16)\n",
    "\n",
    "# second subplot\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.plot(x, runge(x), 'k', label='exact')\n",
    "ax1.margins(0.1)\n",
    "\n",
    "xi = np.linspace(-1, 1, 18)\n",
    "Pchip = si.PchipInterpolator(xi, runge(xi))\n",
    "ax1.plot(x, Pchip(x), 'b', label='PCHIP')\n",
    "\n",
    "ax1.plot(xi, runge(xi), 'ro', label='data')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=16)\n",
    "ax1.set_title('PCHIP approximation to Runge function', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Observations\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "- Clearly within each subinterval the interpolant does not stray beyond the ($y$) bounds of the data at either end of the data (note that the simple p/w linear interpolant also has this property).\n",
    "\n",
    "\n",
    "- Note the impact of the monotonicity property on our ability to represent the peak in the Runge case.\n",
    "\n",
    "\n",
    "- Of course this problem would go away if we had a data point at the middle (which we have if we choose an odd number of data points):\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.0, 1.0, 100)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(x, runge(x), 'k', label='exact')\n",
    "ax1.margins(0.1)\n",
    "\n",
    "xi = np.linspace(-1, 1, 9)\n",
    "Pchip = si.PchipInterpolator(xi, runge(xi))\n",
    "ax1.plot(x, Pchip(x), 'b', label='PCHIP')\n",
    "\n",
    "ax1.plot(xi, runge(xi), 'ro', label='data')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=16)\n",
    "ax1.set_title('PCHIP approximation to Runge function', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare interpolation methods\n",
    "\n",
    "We've discussed a lot of different methods to interpolate data in this lecture. Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the above example data again\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# some of the interpolators we've seen in this lecture, evaluated using \n",
    "# SciPy for consistency but we know that our own codes agree with those we implemented\n",
    "Plinear = si.interp1d(xi, yi, 'linear', fill_value='extrapolate')\n",
    "PL = si.lagrange(xi, yi)\n",
    "Pcs = si.CubicSpline(xi, yi)\n",
    "Pchip = si.PchipInterpolator(xi, yi)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "x = np.linspace(0.4, 9.1, 1000)\n",
    "\n",
    "ax1.plot(x, PL(x), label='Lagrange')\n",
    "ax1.plot(x, Plinear(x), label='p/w linear')\n",
    "ax1.plot(x, Pchip(x), label='PCHIP')\n",
    "ax1.plot(x, Pcs(x), label='cubic spline')\n",
    "\n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('Comparison of interpolation methods', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrature \n",
    "\n",
    "## Review\n",
    "\n",
    "Recall that we are seeking to calculate the area under a curve (assuming a function of one variable)\n",
    "\n",
    "[volume under a surface for a function of two variables] ....\n",
    "\n",
    "$$ I := \\int_{a}^{b} f\\left ( x \\right )\\,dx, $$\n",
    "\n",
    "This is the the *definite* [*integral*](http://en.wikipedia.org/wiki/Integral) over $[a,b]$ of the function $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the fundamental property of a definite integral that is vital for quadrature:\n",
    "\n",
    "$$\\int_{a}^{b} f\\left ( x \\right )\\,dx = \\int_{a}^{c} f\\left ( x \\right )\\,dx + \\int_{c}^{b} f\\left ( x \\right )\\,dx,$$\n",
    "\n",
    "where $c$ is a point between $a$ and $b$. \n",
    "\n",
    "<br>\n",
    "\n",
    "We can extend this to the finite number of points\n",
    "\n",
    "$$ a = x_0\\, < \\,x_1\\,<\\,x_2\\,<\\,\\ldots\\,<\\,x_{n-1}\\,<x_n = b,$$\n",
    "\n",
    "and then our desired integral can be written as, and thus calculated using:\n",
    "\n",
    "$$\\int_{a}^{b} f\\left ( x \\right )\\,dx = \\sum_{i=0}^{n-1}  \\int_{x_i}^{x_{i+1}} f\\left ( x \\right )\\,dx,$$\n",
    "\n",
    "where the $i$-th interval is defined as $[x_i,x_{i+1}]$ and hence $i$ runs from 0 to $n-1$, and \n",
    "\n",
    "$$\\bigcup\\limits_{i=0}^{n-1} \\, [x_i,x_{i+1}] = [a,b].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first quickly review some methods we say in Computational Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Midpoint rule\n",
    "\n",
    "The *midpoint rule* is perhaps the simplest quadrature rule. \n",
    "\n",
    "For reasons that should be obvious from the next figure it is sometimes also called the *rectangle method*.\n",
    "\n",
    "Consider one of the subintervals $\\,[x_i, x_{i+1}].$\n",
    "\n",
    "The midpoint rule approximates the integral over this (the $i$-th) subinterval by the area of a *rectangle*, with a base of length $\\,(x_{i+1}-x_i)\\,$ and a height given by the value of $\\,f(x)\\,$ at the midpoint of that interval (i.e. at $\\,x=(x_{i+1}+x_i)/2$):\n",
    "\n",
    "$$ I_M^{(i)} := (x_{i+1}-x_i) \\;\\; \\times \\;\\; f \\left ( \\frac {x_{i+1}+x_i} {2} \\right ), \\;\\;\\;\\;\\;\\;\\text{for}\n",
    "\\;\\;\\;\\;\\;\\; 0\\le i \\le n-1.$$\n",
    "\n",
    "The midpoint estimate of $I$ then simply involves summing up over all the subintervals:\n",
    "\n",
    "$$I_M := \\sum_{i=0}^{n-1} \\, f \\left ( \\frac {x_{i+1}+x_i} {2} \\right )\\, (x_{i+1}-x_i).$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's write some code to plot the idea as well as compute an estimate of the integral using the midpoint rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a matplotlib function that allows us to easily plot rectangles\n",
    "# which will be useful for visualising what the midpoint rule does\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "\n",
    "# Get the value of pi from numpy and generate equally spaced values from 0 to pi.\n",
    "x = np.linspace(0, np.pi, 100)\n",
    "y = f(x)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.plot(x, y, 'b', lw=2)\n",
    "\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# Label axis.\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$f(x)=\\sin(x)$', fontsize=16)\n",
    "ax1.set_title('Approximating a function with rectangles', fontsize=16)\n",
    "\n",
    "# Overlay a grid.\n",
    "ax1.grid(True)\n",
    "\n",
    "number_intervals = 5\n",
    "xi = np.linspace(0, np.pi, number_intervals+1)\n",
    "I_M = 0.0\n",
    "for i in range(number_intervals):\n",
    "    ax1.add_patch(Rectangle((xi[i], 0.0), (xi[i+1] - xi[i]),\n",
    "                            f((xi[i+1]+xi[i])/2), fill=False, ls='--', color='k', lw=2))\n",
    "    I_M += f((xi[i+1]+xi[i])/2)*(xi[i+1] - xi[i])\n",
    "\n",
    "# use an explicit show here to force the figure to appear before the following print.\n",
    "plt.show()\n",
    "print('The sum of the areas of the rectangles is (I_M): {:.12f}'.format(I_M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def midpoint_rule(a, b, function, number_intervals=10):\n",
    "    \"\"\" Our implementation of the midpoint quadrature rule.\n",
    "    \n",
    "    a and b are the end points for our interval of interest.\n",
    "    \n",
    "    'function' is the function of x \\in [a,b] which we can evaluate as needed.\n",
    "    \n",
    "    number_intervals is the number of subintervals/bins we split [a,b] into.\n",
    "    \n",
    "    Returns the integral of function(x) over [a,b].\n",
    "    \"\"\"\n",
    "    interval_size = (b - a)/number_intervals\n",
    "\n",
    "    # Some examples of some asserts which might be useful here - \n",
    "    # you should get into the habit of using these sorts of checks as much as is possible/sensible.\n",
    "    assert interval_size > 0\n",
    "    assert type(number_intervals) == int\n",
    "    \n",
    "    # Initialise to zero the variable that will contain the cumulative sum of all the areas\n",
    "    I_M = 0.0\n",
    "    \n",
    "    # Find the first midpoint -- i.e. the centre point of the base of the first rectangle\n",
    "    mid = a + (interval_size/2.0)\n",
    "    # and loop until we get past b, creating and summing the area of each rectangle\n",
    "    while (mid < b):\n",
    "        # Find the area of the current rectangle and add it to the running total\n",
    "        # this involves an evaluation of the function at the subinterval midpoint\n",
    "        I_M += interval_size * function(mid)\n",
    "        # Move the midpoint up to the next centre of the interval\n",
    "        mid += interval_size\n",
    "\n",
    "    # Return our running total result\n",
    "    return I_M\n",
    "\n",
    "# check the function runs and agrees with our first version used to generate the schematic plot of the method above:\n",
    "print('midpoint_rule(0, np.pi, np.sin, number_intervals=5) = ', midpoint_rule(0, np.pi, np.sin, number_intervals=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Trapezoidal rule\n",
    "\n",
    "If we change the shape of the rectangle to a trapezoid (i.e. the top of the shape now being a linear line fit defined by the values of the function at the two end points of the subinterval, rather than the constant value used in the midpoint rule), we arrive at the trapezoid, or trapezoidal, rule. \n",
    "\n",
    "The trapezoid rule approximates the integral by the area of a trapezoid with base $(x_{i+1}-x_i)$ and the left- and right-hand-sides equal to the values of the function at the two end points.  \n",
    "\n",
    "In this case the area of the shape approximating the integral over one subinterval, is given by:\n",
    "\n",
    "$$I_T^{(i)} := (x_{i+1}-x_i) \\;\\; \\times \\;\\; \n",
    "\\left( \\frac {f\\left ( x_{i+1}\\right ) + f \\left (x_{i} \\right )} {2} \\right)\n",
    "\\;\\;\\;\\;\\;\\;\\text{for}\n",
    "\\;\\;\\;\\;\\;\\; 0\\le i \\le n-1.$$\n",
    "\n",
    "The trapezoidal estimate of $I$ then simply involves summing up over all the subintervals:\n",
    "\n",
    "$$I_T := \\sum_{i=0}^{n-1}\\,   \\left(\\frac{f(x_{i+1}) + f(x_{i})}{2}\\right )\\, (x_{i+1}-x_i). $$\n",
    "\n",
    "Let's write some code to plot the idea and compute an estimate of the integral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a matplotlib function that allows us to plot polygons\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "\n",
    "# Get the value of pi from numpy and generate equally spaced values from 0 to pi.\n",
    "x = np.linspace(0, np.pi, 100)\n",
    "y = f(x)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.plot(x, y, 'b', lw=2)\n",
    "\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# Label axis.\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$\\sin(x)$', fontsize=16)\n",
    "ax1.set_title('Approximating function with trapezoids', fontsize=16)\n",
    "\n",
    "# Overlay a grid.\n",
    "ax1.grid(True)\n",
    "\n",
    "number_intervals = 5\n",
    "xi = np.linspace(0, np.pi, number_intervals+1)\n",
    "I_T = 0.0\n",
    "for i in range(number_intervals):\n",
    "    ax1.add_patch(Polygon(np.array([[xi[i], 0], [xi[i], f(xi[i])], [\n",
    "                  xi[i+1], f(xi[i+1])], [xi[i+1], 0]]), closed=True, fill=False, ls='--', color='k', lw=2))\n",
    "    I_T += ((f(xi[i+1]) + f(xi[i]))/2)*(xi[i+1] - xi[i])\n",
    "\n",
    "plt.show()\n",
    "print('The sum of the areas of the trapezoids is (I_T): {:.12f}'.format(I_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The composite version of the trapezoidal rule\n",
    "\n",
    "Of course if we think about what we're doing here we recognise that we can save some computational time (specifically function evaluations, which could each be very expensive so in some situations this may be especially worthwhile.).\n",
    "\n",
    "Above we wrote down the mathematical expression for the scheme:\n",
    "\n",
    "$$I_T := \\sum_{i=0}^{n-1}\\,   \\left(\\frac{f(x_{i+1}) + f(x_{i})}{2}\\right )\\, (x_{i+1}-x_i),$$\n",
    "\n",
    "(and this is *exactly* what we implemented in our code).\n",
    "\n",
    "But notice that under the assumption that the $x$ spacing of the data is uniform (say, $\\Delta x$) this is exactly equivalent to\n",
    "\n",
    "$$I_T := [f(x_0) + 2f(x_1) + 2f(x_2) + \\ldots + 2f(x_{n-1}) + f(x_n)]\\frac{\\Delta x}{2},$$\n",
    "\n",
    "which emphasises that we can implement a version of the rule which keeps the number of function evaluations to a minimum, i.e. which reduces the cost of the naive implementation.  \n",
    "\n",
    "Note that this idea (minimising function evaluations) can of course also be generalised to the case where the $x$ points are not equally spaced, we just can't write the final expression in such a simple form (specifically we can't pull out the factor $\\Delta x$ and have just \"1\" and \"2\" multipliers of the function values).\n",
    "\n",
    "The method implemented in this manner is termed the *composite trapezoidal rule*.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_trapezoidal_rule(a, b, function, number_intervals=10):\n",
    "    \"\"\"Our implementation of the trapezoidal quadrature rule - composite version\"\"\"\n",
    "    interval_size = (b - a)/number_intervals\n",
    "    assert interval_size > 0\n",
    "    assert type(number_intervals) == int\n",
    "    I_T = 0.0\n",
    "    # Use the composite form of the rule\n",
    "    # note that with some thought we can minimise the multiplications by 2, as well as by\n",
    "    # the interval_size:\n",
    "    I_T += function(a)/2.0\n",
    "    for i in range(1, number_intervals):\n",
    "        I_T += function(a + i * interval_size)\n",
    "    I_T += function(b)/2.0\n",
    "    # Return our running total result\n",
    "    return I_T * interval_size\n",
    "\n",
    "print('composite_trapezoidal_rule(0, np.pi, np.sin, number_intervals=5) = ', \n",
    "      composite_trapezoidal_rule(0, np.pi, np.sin, number_intervals=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quadrature Accuracy and Errors\n",
    "\n",
    "Another observation here is that in this particular case of half a sine wave, the trapezoid rule always *under-estimates* the area, whereas the midpoint rule *over-estimates*. \n",
    "\n",
    "We noted that, perhaps surprisingl, the midpoint rule is more accurate than the trapezoid rule - the reason for this is not immediately obvious from the discussions and the images above.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that the accuracy of a quadrature rule is predicted by examining its behaviour in practice with *polynomials*. \n",
    "\n",
    "We say that the **degree of accuracy** or the **degree of precision** of a quadrature rule is equal to $M$ if it is exact for all polynomials of degree up to and including $M$, but not exact for some polynomial of degree $M+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clearly both the midpoint and trapezoid rules will give the exact result for both constant and linear functions,\n",
    "\n",
    "but they are not exact for quadratics \n",
    "\n",
    "[you could test our codes yourself on the function $x^2$ to demonstrate this].\n",
    "\n",
    "Therefore, they both have a degree of precision of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error analysis (qualitative)\n",
    "\n",
    "For the \"concave-down\" (i.e. the first half of a sine wave) function we chose above, notice from the plot that the trapezoidal rule will **consistently underestimate** the area under the curve, as the line segments approximating the function are always under the concave function curve.\n",
    "\n",
    "In contrast, the mid-point rule will have parts of each rectangle above and below the curve, hence to a certain extent the **errors will cancel** each other out. \n",
    "\n",
    "This is why, *for this particular example*, the errors in the mid-point rule turn out to be approximately half those in the trapezoidal rule.  \n",
    "\n",
    "While this result turns out to be *generally* true for smooth functions, we can always come up with (counter) examples where the trapezoid rule will win (can you think of an example?).\n",
    "\n",
    "Taylor series analysis can be used to formally construct upper bounds on the quadrature error for both methods. \n",
    "\n",
    "We know that the error when integrating constant and linear functions is zero for our two rules, so let's first consider an example of integrating a quadratic polynomial.\n",
    "\n",
    "We know analytically that\n",
    "\n",
    "$$\\int_{0}^{1} x^{2}\\,dx = \\left.\\frac{1}{3}x^3\\right|_0^1=\\frac {1}{3}.$$\n",
    "\n",
    "Whereas numerically the midpoint rule on a single interval gives an approximation of\n",
    "\n",
    "\\begin{equation}\n",
    "I_M = 1 \\left(\\frac {1}{2}\\right)^{2} = \\frac {1}{4},\n",
    "\\end{equation}\n",
    "\n",
    "while the trapezoidal rule gives\n",
    "\n",
    "\\begin{equation}\n",
    "I_T = 1 \\frac {0+1^{2}}{2} = \\frac {1}{2}.\n",
    "\\end{equation}\n",
    "\n",
    "The error for $I_M$ is therefore $1/3 - 1/4 = 1/12$, while the error for $I_T$ is $1/3 - 1/2 = -1/6$.\n",
    "\n",
    "Therefore, the midpoint rule is twice as accurate as the trapezoid rule:\n",
    "\n",
    "$$|E_M| = \\frac{1}{2} |E_T|,$$\n",
    "\n",
    "where $|E|$ indicates the error (the absolute value of the difference from the exact solution).\n",
    "\n",
    "This is the case for this simple example, and we can see from the actual error values printed above that it also appears to be approximately true for the sine (which note is not a simple polynomial) case as well.\n",
    "\n",
    "We will make use of this knowledge to generate new more accurate quadrature rules below [can you think of a way we might try to do this?], but first let's sketch how you can go about a more rigorous analysis/estimation of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Error analysis (mathematical) [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "We are approximating\n",
    "\n",
    "$$ I := \\int_{a}^{b} f\\left ( x \\right )\\,dx, $$\n",
    "\n",
    "with a quadrature rule of the form (in the case of the Trapezoidal rule)\n",
    "\n",
    "$$I_T :=  \\left(\\frac{f(b) + f(a)}{2}\\right )\\, (b-a).$$\n",
    "\n",
    "\n",
    "More generally (and we will develop this idea formally below where we talk about the family of Newton-Cotes quadrature rules) what we are actually doing with the midpoint and trapezoidal rules is first approximating the function $f(x)$ with a *polynomial interpolant*, $P_N(x)$, over each (sub-)interval, and our approximation to the integral is given by \n",
    "\n",
    "\n",
    "$$\n",
    "I_N := \\int_a^b\\,P_N(x)\\,dx  \\approx I,\n",
    "$$\n",
    "\n",
    "where $P_N(x)$ is the degree $N$ interpolating polynomial, and for which due to its simple form we are able to write down an expression which evaluates its integral exactly.  For the midpoint case $N=0$, while for the trapezoidal case $N=1$.\n",
    "\n",
    "Note that there is a bit of **scope for confusion** here as we are clearly going to have (at least) two possibilities to try and improve the accuracy of our approximate integral: change the number of subintervals ($n$), or the polynomial order ($N$). \n",
    "\n",
    "Let's assume for the time being that we have a single interval that spans all of $[a,b]$, i.e. assume that $n=1$.\n",
    "\n",
    "Later we will apply our analysis to a subinterval $[x_i,x_{i+1}]$ and sum the corresponding subinterval errors to obtain an error for the version of each quadrature rule evaluated over multiple sub-intervals.\n",
    "\n",
    "Recall from earlier that the error in polynomial interpolation can be written as\n",
    "\n",
    "$$ f(x) - P_N(x) =: R_N(x) = \\frac{(x-x_0)(x-x_1)\\cdots(x-x_N)}{(N+1)!} f^{(N+1)}(c_x) = \\Psi_N(x)\\frac{f^{(N+1)}(c_x)}{(N+1)!}, $$\n",
    "\n",
    "where $c_x$ is *some* value/point between the maximum and minimum of $x_0, \\ldots, x_N$ and $x$,\n",
    "\n",
    "and where \n",
    "\n",
    "$$ \\Psi_N(x) := \\prod^N_{i=0} (x - x_i) = (x-x_0)(x-x_1)\\cdots(x-x_N). $$\n",
    "\n",
    "This means that our quadrature error satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_N \n",
    "&= I - I_N = \\int_{a}^{b} f\\left ( x \\right )\\, dx - \\int_a^b\\,P_N\\left ( x \\right )\\,dx \\\\[5pt]\n",
    "&= \\int_{a}^{b} f\\left ( x \\right ) - P_N\\left ( x \\right ) \\, dx\\\\[5pt] \n",
    "&= \\int_{a}^{b} R_N\\left ( x \\right )\\, dx.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Trapezoidal rule error (one interval)\n",
    "\n",
    "The trapezoidal rule follows from taking $N=1$ in which case we have the interpolation error over the interval $[a,b]$:\n",
    "\n",
    "$$R_1(x) = (x - a) (x - b) \\frac{f''(c_x)}{2},$$\n",
    "\n",
    "for some $c_x\\in[a,b]$, and thus \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_T \\equiv E_1 \n",
    "& = \\int_a^b (x - a) (x - b) \\frac{f''(c_x)}{2} \\,dx \\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{2}  \\int_a^b (x - a) (x - b) \\,dx \\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{2}  \\int_a^b x^2 - (a+b)x + ab  \\,dx \\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{2}  \\left[\\frac{1}{3}x^3 - \\frac{(a+b)}{2}x^2 + abx \\right]_a^b\\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{2}  \\left(\\frac{1}{3}(b^3 - a^3) - \\frac{(a+b)}{2}(b^2 - a^2) + ab(b - a) \\right)\\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{12}  \\left(2 (b^3 - a^3) - 3(a+b)(b^2 - a^2) + 6ab(b - a) \\right)\\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{12}  \\left(2 (b^3 - a^3) - 3(b^3 - a^3 +ab^2-a^2b)+ 6ab^2 - 6a^2b \\right)\\\\[5pt]\n",
    "& = \\frac{f''(c_x)}{12}  \\left( a^3 - 3a^2b + 3ab^2 -b^3 \\right)\\\\[5pt]\n",
    "& = -\\frac{(b-a)^3}{12} f''(c_x).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Observations\n",
    "\n",
    "\n",
    "- If $f''>0$ (i.e. the function is concave up), then this tells us that the error is negative (as we assume $b>a$) and the trapezoidal rule in this case over-estimates the true value.\n",
    "\n",
    "\n",
    "- Similarly, if it is concave down ($f''<0$) then the estimate is an under-estimate (as we saw above).\n",
    "\n",
    "\n",
    "- Over a single interval the error is cubically dependent on the interval size (interval_size = $b-a$).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Trapezoidal rule error (multiple sub-intervals)\n",
    "\n",
    "The trapezoidal rule over multiple sub-intervals took the form\n",
    "\n",
    "$$ \\int_{a}^{b} f\\left ( x \\right )\\, dx \\approx \\sum_{i=0}^{n-1}\\,   \\left(\\frac{f(x_{i+1}) + f(x_{i})}{2}\\right )\\, (x_{i+1}-x_i),$$\n",
    "\n",
    "where the LHS can be written as\n",
    "\n",
    "$$ I := \\int_{a}^{b} f\\left ( x \\right )\\, dx = \\sum_{i=0}^{n-1}\\,   \\int_{x_i}^{x_{i+1}}\\,f\\left ( x \\right )\\,dx, $$\n",
    "\n",
    "and the RHS is equivalent to \n",
    "\n",
    "$$ I_T^n \\equiv I_1^n := \\sum_{i=0}^{n-1}\\, \\int_{x_i}^{x_{i+1}} \\,P_1 \\left ( x \\right )\\,dx, $$\n",
    "\n",
    "the superscript $n$ here is used to remind us that we are dealing with the version of the rule over $n$ subintervals.\n",
    "\n",
    "The error is therefore\n",
    "\n",
    "$$ E_1^n = I - I_1^n = \\sum_{i=0}^{n-1}\\, \\int_{x_i}^{x_{i+1}} \\, f \\left ( x \\right ) - P_1 \\left ( x \\right )\\,dx, $$\n",
    "\n",
    "i.e. the sum of the errors in the integral of the function over each subinterval.  But we can reapply our result above on the error over an arbitrary interval $[a,b]$ to the subinterval $[x_i,x_{i+1}]$ to obtain\n",
    "\n",
    "$$ E_T^n \\equiv E_1^n = -\\sum_{i=0}^{n-1}\\, \\frac{(x_{i+1}-x_i)^3}{12} f''\\left(c_{x_i}\\right), $$\n",
    "\n",
    "for some $c_{x_i}\\in[x_i,x_{i+1}]$.\n",
    "\n",
    "If we assume uniform intervals then $x_{i+1}-x_i = \\Delta x$ and $n \\Delta x = (b-a)$, and so we can rewrite this as\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "E_1^n & = -\\sum_{i=0}^{n-1}\\, \\frac{(x_{i+1}-x_i)^3}{12} f''\\left(c_{x_i}\\right) \\\\[5pt]\n",
    "& = - \\sum_{i=0}^{n-1}\\, \\frac{\\Delta x^3}{12} f''\\left(c_{x_i}\\right) \\\\[5pt]\n",
    "& = - \\frac{\\Delta x^3}{12} \\sum_{i=0}^{n-1}\\,f''\\left(c_{x_i}\\right) \\\\[5pt]\n",
    "& = - \\frac{\\Delta x^2}{12} \\,\\Delta x\\, \\sum_{i=0}^{n-1}\\,f''\\left(c_{x_i}\\right) \\\\[5pt]\n",
    "& = - \\frac{\\Delta x^2}{12} \\,\\frac{(b-a)}{n}\\, \\sum_{i=0}^{n-1}\\,f''\\left(c_{x_i}\\right) \\\\[5pt]\n",
    "&= -\\frac{1}{12} \\Delta x^2 (b-a) \\frac{1}{n} \\sum_{i=0}^{n-1}\\,  f''\\left(c_{x_i}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can interpret the $(1/n)\\sum_{i=0}^{n-1}\\,  f''\\left(c_{x_i}\\right)$ term, where $c_{x_i}$ for each $i$ is some unknown value within each subinterval, as being a kind of average value for the second-derivative over our *full* interval $[a,b]$, and as $n$ is increased it converges to the average value (basically because there is a tighter and tighter constraint on each of the unknown $c_{x_i}$ values).  \n",
    "\n",
    "This expression then indeed proves that the Trapezoidal rule is second-order accurate in the interval size $\\Delta x$ (or equivalently the number of subintervals).\n",
    "\n",
    "Note that we can turn this error estimate into an *error bound* by replacing the average value of the second-derivative with its maximum absolute value (an average of as set always being less than or equal to its maximum absolute value):\n",
    "\n",
    "$$ \\left| E_1^n \\right| \\le \\frac{1}{12} \\Delta x^2 (b-a) \\max_{x\\in[a,b]}\\, \\left| f''(x) \\right|, $$\n",
    "\n",
    "or as $n \\Delta x = (b-a)$\n",
    "\n",
    "$$ \n",
    "\\boxed{\\left| E_1^n \\right| \\le \\frac{(b-a)^3}{12n^2}  \\max_{x\\in[a,b]}\\, \\left| f''(x) \\right|} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note further that in the limit of a large number of subintervals the mathematical expression for the average value of the second-derivative actually converges to the integral of $f''$ over $[a,b]$, divided by $(b-a)$, which by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#Second_part) is equal to $(f'(b) - f'(a))/(b-a)$ and therefore in this asymptotic limit we have\n",
    "\n",
    "$$ E_1^n = -\\frac{1}{12} \\Delta x^2 \\left( f'(b) - f'(a) \\right). $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Midpoint rule error\n",
    "\n",
    "We won't go through the same derivation for the midpoint rule here (but see the homework exercise).\n",
    "\n",
    "We will simply state that the formal analysis for the midpoint rule indeed arrives at an error estimate which is exactly half that of the Trapezoidal rule, i.e.\n",
    "\n",
    "$$ \n",
    "\\boxed{\n",
    "\\left| E_M^n \\right| \\le \\frac{(b-a)^3}{24n^2}  \\max_{x\\in[a,b]}\\, \\left| f''(x) \\right|} \n",
    "$$\n",
    "\n",
    "See [https://en.wikipedia.org/wiki/Riemann_sum](https://en.wikipedia.org/wiki/Riemann_sum) for \"confirmation\" of both of these error bounds - both Midpoint and Trapezoid are listed under the Methods section.\n",
    "\n",
    "<br>\n",
    "    \n",
    "Note also that, as argued above, the actual signs of the Midpoint and Trapezoidal rule errors will be opposite to one another.\n",
    "\n",
    "This hints to the fact that if we sum suitable multiples of the two estimates of the integral we obtain for Midpoint and Trapezoidal, we should to a certain extent be able to cancel out this error. We will do this next.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simpson's rule [Review]\n",
    "\n",
    "Knowing the error estimates from the two rules explored so far opens up the potential for us to combine them in an appropriate manner to create a new quadrature rule, generally more accurate than either one separately. \n",
    "\n",
    "Suppose $I_S$ indicates an unknown, but more accurate, estimate of the integral over an interval.  \n",
    "\n",
    "Then, as seen above, as $I_T$ has an error that is approximately $-2$ times the error in $I_M$, the following relation must hold approximately:\n",
    "\n",
    "\n",
    "$$I_S - I_T \\approx -2 \\left ( I_S - I_M\\right ).$$\n",
    "\n",
    "\n",
    "This follows from the fact that $\\,I - I_T \\approx -2 \\left ( I - I_M\\right )$, provided that $I_S$ is closer to $I$ than either of the other two estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Replacing this approximately equals sign with actual equality defines $I_S$ for us in terms of things we know. \n",
    "\n",
    "We can rearrange this to give an expression for $I_S$ that yields a more accurate estimate of the integral than either $I_M$ or $I_T$:\n",
    "\n",
    "$$I_S := \\frac{2}{3}I_M + \\frac{1}{3}I_T.$$\n",
    "\n",
    "What we're doing here is using the fact that we know something about (the *leading order* behaviour of the) two errors, and we can therefore combine them to cancel this error to a certain extent.\n",
    "\n",
    "This estimate will generally be more accurate than either $M$ or $T$ alone. The error won't actually be zero in general as we're only cancelling out the leading order term in the error, but a consequence is that we will be left with higher-degree terms in the error expansion of the new quadrature rule which should be smaller (at least in the asymptotic limit), and converge faster. \n",
    "\n",
    "The resulting quadrature method in this case is known as [Simpson's rule](http://en.wikipedia.org/wiki/Simpson%27s_rule):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I_S &:= \\frac{2}{3}I_M + \\frac{1}{3}I_T \\\\[5pt]\n",
    "&= \\frac{2}{3}  (b-a)f\\left ( \\frac{a+b}{2}\\right ) + \\frac{1}{3}(b-a)\\frac{(f(a) + f(b))}{2} \\\\[5pt]\n",
    "& = \\frac{(b-a)}{6}\\left( f \\left ( a\\right ) + 4f \\left ( c\\right ) + f\\left ( b\\right )\\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a$ and $b$ are the end points of an interval and $c = \\left ( a+b\\right )/2$ is the midpoint.\n",
    "\n",
    "\n",
    "Note that an alternate derivation of the same rule involves fitting a *quadratic function* (i.e. $P_2(x)$ rather than the constant and linear approximations already considered) that interpolates the integral at the two end points of the interval, $a$ and $b$, as well as at the midpoint, $c = \\left ( a+b\\right )/2$, and calculating the integral under that polynomial approximation.\n",
    "\n",
    "See the homework exercise, and note that we'll come back to this idea a bit later when we introduce the Newton-Cotes family of quadrature rules.\n",
    "\n",
    "Let's plot what this method is doing and compute the integral for our sine case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a matplotlib function that allows us to plot polygons\n",
    "# use this to plot the straight sides, and add an approximate\n",
    "# quadratic at the top.\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# Get the value of pi from numpy and generate equally spaced values from 0 to pi.\n",
    "x = np.linspace(0, np.pi, 100)\n",
    "y = f(x)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.plot(x, y, 'b', lw=2)\n",
    "\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# Label axis.\n",
    "ax1.set_xlabel('x', fontsize=16)\n",
    "ax1.set_ylabel('sin(x)', fontsize=16)\n",
    "ax1.set_title('Approximating a function with shapes with quadratic tops', fontsize=16)\n",
    "\n",
    "# Overlay a grid.\n",
    "ax1.grid(True)\n",
    "\n",
    "number_intervals = 5\n",
    "xi = np.linspace(0, np.pi, number_intervals+1)\n",
    "\n",
    "I_S = 0.0\n",
    "\n",
    "for i in range(number_intervals):\n",
    "    # use a non-closed Polygon to visualise the straight sides of each interval \n",
    "    ax1.add_patch(Polygon(np.array([[xi[i], f(xi[i])], [xi[i], 0], [xi[i+1], 0], [xi[i+1], f(xi[i+1])]]),\n",
    "                          closed=False, fill=False, ls='--', color='k', lw=2))\n",
    "    # add the quadratic top - fit a quadratic using numpy\n",
    "    poly_coeff = np.polyfit((xi[i], (xi[i] + xi[i+1])/2.0, xi[i + 1]),\n",
    "                            (f(xi[i]), f((xi[i] + xi[i+1])/2.0), f(xi[i+1])), 2)\n",
    "    p1 = np.poly1d(poly_coeff)\n",
    "    # plot the quadratic using 20 plotting points within the interval \n",
    "    ax1.plot(np.linspace(xi[i], xi[i+1], 20),\n",
    "             p1(np.linspace(xi[i], xi[i+1], 20)), ls='--', color='k', lw=2)\n",
    "    # add in the area of the interval shape to our running total using Simpson's formula\n",
    "    I_S += ((xi[i+1] - xi[i])/6.) * (f(xi[i]) + 4 *\n",
    "                                     f((xi[i] + xi[i+1])/2.0) + f(xi[i+1]))\n",
    "\n",
    "plt.show()\n",
    "print(\"The Simpson's rule approximation (I_S): {:.12f}\".format(I_S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing Simpson's rule\n",
    "\n",
    "Note in the above image the approximation of the quadratic over each subinterval is very good - visually it looks exact but of course it isn't as we can exactly approximate a trigonometric function with a polynomial.\n",
    "\n",
    "This is confirmed by the very accurate approximation to the integral. \n",
    "\n",
    "But note that here we have had to additionally evaluate the function at the midpoints of the subintervals; we need to bear this in mind when comparing methods. With 5 subintervals the number of function evaluations we have used so far is: Midpoint - 5; Trapezoidal (the composite version) - 6; Simpson's - 11 (that's for the naive approach where we don't re-use function evaluations).\n",
    "\n",
    "Of course the agreement (visually and quantitatively) won't be quite so good for complicated, rapidly varying functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The composite version of Simpson's Rule\n",
    "\n",
    "If we assume that our interval $[a,b]$ has been split up into $n$ intervals (or $n+1$ data points) we can save some function evaluations by writing Simpson's Rule in the following form (note here we do not introduce any additional midpoint function evaluations)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I_{S} \n",
    "& = \\frac{\\Delta x}{3}\\left[ f \\left ( x_0\\right ) + 4f \\left ( x_1\\right ) + 2f\\left ( x_2\\right ) + 4f \\left ( x_3\\right ) + \\cdots + 2 f \\left ( x_{n-2}\\right )  + 4 f \\left ( x_{n-1}\\right ) +  f \\left ( x_{n}\\right ) \\right]\\\\[5pt]\n",
    "& = \\frac{\\Delta x}{3}\\left[ f \\left ( x_0\\right ) + 2\\sum_{i=1}^{n/2 - 1} f\\left(x_{2i}\\right) + 4\\sum_{i=1}^{n/2} f\\left(x_{2i-1}\\right)  +  f \\left ( x_{n}\\right ) \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is known as the [Composite Simpson's rule](http://en.wikipedia.org/wiki/Simpson%27s_rule#Composite_Simpson.27s_rule), \n",
    "or more precisely the *composite Simpson's 1/3 rule*.\n",
    "\n",
    "This is the version of Simpson's rule implemented by Scipy [`scipy.interpolate.simps`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simps.html).\n",
    "\n",
    "\n",
    "Note that this way of formulating Simpson's rule (where we do not allow additional function evaluations at the midpoints of intervals - we assume we are only in a position to use the given data points) requires that $n$ be even.\n",
    "\n",
    "This way of writing the composite form in the case of $n=2$ is equivalent to the formula over $[a,b]$ that introduced the additional midpoint location $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def composite_simpsons_rule(a, b, function, number_intervals=10):\n",
    "    \"\"\"Function to evaluate the composite Simpson's rule only using\n",
    "    function evaluations at (number_intervals + 1) points.\n",
    "    \n",
    "    This implementation requires that the number of subintervals (number_intervals) be even\n",
    "    \"\"\"\n",
    "    assert number_intervals % 2 == 0, \"number_intervals is not even\"\n",
    "\n",
    "    interval_size = (b - a) / number_intervals\n",
    "    # start with the two end member values\n",
    "    I_cS2 = function(a) + function(b)\n",
    "\n",
    "    # add in those terms with a coefficient of 4\n",
    "    for i in range(1, number_intervals, 2):\n",
    "        I_cS2 += 4 * function(a + i * interval_size)\n",
    "\n",
    "    # and those terms with a coefficient of 2\n",
    "    for i in range(2, number_intervals-1, 2):\n",
    "        I_cS2 += 2 * function(a + i * interval_size)\n",
    "\n",
    "    return I_cS2 * (interval_size / 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# Now let's test the Simpson's rule function.\n",
    "print(\"The exact area found by direct integration = 2\")\n",
    "interval_sizes = [2, 4, 8, 16, 32, 100, 1000]\n",
    "errors_S = np.zeros_like(interval_sizes, dtype='float64')\n",
    "areas_S = np.zeros_like(interval_sizes, dtype='float64')\n",
    "areas_M = np.zeros_like(interval_sizes, dtype='float64')\n",
    "errors_M = np.zeros_like(interval_sizes, dtype='float64')\n",
    "areas_T = np.zeros_like(interval_sizes, dtype='float64')\n",
    "errors_T = np.zeros_like(interval_sizes, dtype='float64')\n",
    "for (i, number_intervals) in enumerate(interval_sizes):\n",
    "    areas_S[i] = composite_simpsons_rule(0, np.pi, f, number_intervals)\n",
    "    errors_S[i] = abs(areas_S[i] - 2)\n",
    "    print('Area {:<4d} for Simpson = {:.16f} (error = {:.9e})'.format(\n",
    "        number_intervals, areas_S[i], errors_S[i]))\n",
    "    areas_M[i] = midpoint_rule(0, np.pi, f, number_intervals)\n",
    "    errors_M[i] = abs(areas_M[i]-2)\n",
    "    areas_T[i] = composite_trapezoidal_rule(0, np.pi, f, number_intervals)\n",
    "    errors_T[i] = abs(areas_T[i]-2)      \n",
    "    \n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.loglog(interval_sizes, errors_S, 'ro-', lw=2, label='Simpson')\n",
    "ax1.loglog(interval_sizes, errors_T, 'bo-', lw=2, label='Trapezoidal')\n",
    "ax1.loglog(interval_sizes, errors_M, 'ko-', lw=2, label='Midpoint')\n",
    "ax1.set_xlabel('log(number_intervals)', fontsize=16)\n",
    "ax1.set_ylabel('log(error)', fontsize=16)\n",
    "ax1.set_title('Quadrature rule convergence', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "\n",
    "annotation.slope_marker((5e1, 2e-4), (-2, 1), invert=True, ax=ax1, \n",
    "                        size_frac=0.25, pad_frac=0.05, text_kwargs = dict(fontsize = 14))\n",
    "annotation.slope_marker((5e1, 5e-9), (-4, 1), invert=True, ax=ax1, \n",
    "                        size_frac=0.25, pad_frac=0.05, text_kwargs = dict(fontsize = 14));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Observations\n",
    "\n",
    "\n",
    "- The errors are lower than for the midpoint and trapezoidal rules, and the method converge more rapidly - i.e. the relative improvement only gets better for more subintervals.\n",
    "\n",
    "\n",
    "- This expression now integrates up to cubics exactly (by construction), so it has order of precision 3 (with convergence order 4 as confirmed by the convergence plot above).\n",
    "\n",
    "\n",
    "- We're getting down to errors close to machine precision now when we use 1000 subintervals. But remember we may well either have a relatively small number of data points, or want to minimise the number of function evaluations well below this relatively high number. This will mean that for problems with lots of variation, and/or in higher dimensions, that we still work to do in improving our quadrature methods.\n",
    "\n",
    "\n",
    "- Note that there is a little bit of ambiguity over what the number of sub-intervals and number of function evaluations there are in our implementation. Here we do not evaluate the function at the mid point of an interval, instead we combine two intervals to give us the required 3 data points. Mainly we do this to agree with the behaviour of SciPy. We need to remember this when we plot on the $x$ axis either number of intervals (i.e. what exactly  do we mean by this), or if we plot number of function evaluations.\n",
    "\n",
    "\n",
    "We can check that our implementation agrees with SciPy's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "print(\"The area found by direct integration = 2\")\n",
    "for i in (2, 4, 8, 16, 32, 100, 1000):\n",
    "    area = composite_simpsons_rule(0, np.pi, f, i)\n",
    "    print('Area {0:<4d} interval(s), {1:<4d} function evaluations = {2:.16f} (error = {3:.16e})'.format(\n",
    "        i, i+1, area, abs(area-2)))\n",
    "\n",
    "\n",
    "print(\"\\nLet's call the SciPy routine to check we get ~ the same answers:\")\n",
    "for i in (2, 4, 8, 16, 32, 100, 1000):\n",
    "    # we have to give the SciPy function i+1 data points, \n",
    "    # to match the i subintervals specified in our implementation\n",
    "    print(integrate.simps(f(np.linspace(0, np.pi, i+1)),\n",
    "                   np.linspace(0, np.pi, i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Simpson's rule error [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "We can go through the exact same mathematical steps as for the trapezoidal rule above to compute the error in Simpson's rule applied over the full interval $[a,b]$ (i.e. the error over a single interval):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_S \n",
    "&= I - I_2 \\\\[5pt]\n",
    "&= \\int_{a}^{b} \\, f\\left ( x \\right )\\, dx - \\int_a^b\\,P_2\\left ( x \\right )\\,dx \\\\[5pt]\n",
    "&= \\int_{a}^{b} \\, f\\left ( x \\right ) - P_2\\left ( x \\right ) \\, dx\\\\[5pt] \n",
    "&= \\int_{a}^{b} \\, R_2\\left ( x \\right )\\, dx \\\\[5pt] \n",
    "&\\vdots \\\\[5pt]\n",
    "&=-\\frac{1}{90}\\left(\\frac{b-a}{2}\\right)^5 f^{(4)}\\left(c_x\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "[Remember that a superscipt number in brackets indicates a derivative - here the fourth derivative of $f$.]\n",
    "\n",
    "Note this isn't a mistake - we have gained one order of accuracy over what might have been expected due to cancellation of the leading order term.\n",
    "\n",
    "This is *a* reason for the popularity of Simpson's rule.\n",
    "\n",
    "Note that all Newton-Cotes quadrature rules (introduced in a few cells) of even order have this additional order of accuracy property.\n",
    "\n",
    "Summing up over $n$ subintervals yields the error bound\n",
    "\n",
    "$$ \n",
    "\\boxed{\n",
    "\\left| E_S^n \\right| \\;\\; \\le \\;\\;\\frac{\\Delta x^4}{180}  (b-a) \\max_{x\\in[a,b]}\\, \\left| f^{(4)}(x) \\right| \\;\\; =  \\;\\;\\frac{(b-a)^5}{180n^4}  \\max_{x\\in[a,b]}\\, \\left| f^{(4)}(x) \\right|\n",
    "}$$\n",
    "\n",
    "where the $\\Delta x$ and $n$ here refer to the spacing between function evaluations, and the corresponding number of subintervals (i.e. the approach we took in `composite_simpsons_rule` where we do not perform additional function evaluations at the midpoints of subintervals, but rather span two subintervals).\n",
    "\n",
    "\n",
    "The presence of the fourth derivative in the leading order error shows that Simpson integrates third-order polynomials exactly, i.e. its degree of precision is 4.\n",
    "\n",
    "The presence of the $1/n^4$ factor shows that for every doubling of $n$, or halving of $\\Delta x$, the error drops by a factor of 16 (i.e. $2^4$) - the method is fourth-order accurate. This agrees with what we saw in the convergence plot above.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Weddle's rule (extrapolated Simpson's rule) [$\\star$]\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "We noted above that Simpson's rule is fourth-order accurate.\n",
    "\n",
    "Suppose we take an approximation to $I$ using $n$ intervals with Simpson's rule and call the result $I_S$, and then apply Simpson's rule with double the number of intervals ($2n$) and call the result $I_{S_2}$. \n",
    "\n",
    "Then we have two estimates for the integral where we expect $I_{S_2}$ to be approximately $2^4=16$ times more accurate than $S$. In particular, we expect the lowest (i.e. the leading) order error term in $I_{S_2}$ to be precisely one sixteenth that of $I_S$.\n",
    "\n",
    "Similar to how we derived Simpson's rule by combining what we knew of the error for the midpoint and trapezoidal rules, with this knowledge we can combine the two estimates from Simpson's rule to derive an even more accurate estimate of $I$:\n",
    "\n",
    "Let's call this more accurate rule $I_W$, which we can find by solving:\n",
    "\n",
    "$$I_W - I_S = 16 \\left ( I_W - I_{S_2} \\right ),$$\n",
    "\n",
    "for $I_W$.\n",
    "\n",
    "A bit of manipulation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\;\\;\\; I_W - I_S = 16 \\left ( I_W - I_{S_2} \\right ) \\\\[5pt]\n",
    "\\implies & \\;\\;\\; I_W - I_S = 16 I_W - 16 I_{S_2} \\\\[5pt]\n",
    "\\implies & \\;\\;\\; 15 I_W  = 16 I_{S_2} - I_S \\\\[5pt]\n",
    "\\implies & \\;\\;\\; 15 I_W  = 15 I_{S_2} + (I_{S_2} - I_S) ,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "leads us to the expression\n",
    "\n",
    "$$ I_W = I_{S_2} + \\frac {\\left (I_{S_2} - I_S \\right )}{15}.$$\n",
    "\n",
    "This is known as *Weddle's rule*, or the *extrapolated Simpson's rule* because it uses two different values for the interval size and *extrapolates* from these two to obtain an even more accurate result. \n",
    "\n",
    "Making a function for this rule is easy as we can just call our Simpson's rule functions with two values for the number of intervals.\n",
    "\n",
    "Doing this is a homework exercise and should yield the following updated convergence plot\n",
    "\n",
    "```{figure} Figures/weddle_convergence.png\n",
    ":width: 75%\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Observations\n",
    "\n",
    "- You can see our final rule is much more accurate for fewer required intervals. \n",
    "\n",
    "\n",
    "- Indeed we are down at the limits where round-off errors are clearly affecting our results and impacting on convergence rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Newton-Cotes quadrature rules [$\\star\\star$]\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "Taking the idea behind Simpson's rule, which can be interpreted as (or derived by) fitting a quadratic Lagrange interpolating polynomial to *equally spaced* points over an interval, and extending to any order Lagrange polynomial leads to the *Newton-Cotes* family of quadrature rules.\n",
    "\n",
    "[Of course the midpoint and trapezoidal rules fit within this description as well with the fitting of a constant and a linear polynomial.]\n",
    "\n",
    "Assume we have $N+1$ equally spaced data points $(x_i,y_i)$ [we will see examples where these points are not fixed in advance below]. We will assume we are dealing with the integration of a known function and so $y_i = f(x_i)$.\n",
    "\n",
    "**Note we are trying to be careful about the distinction between $n$ and $N$ throughout this lecture!**\n",
    "\n",
    "We know from earlier that we can fit an order $N$ polynomial exactly through these points; we call this  polynomial $P_N(x)$.\n",
    "\n",
    "Recall the Lagrange form of $P_N$ was defined as\n",
    "\n",
    "$$L(x) := \\sum_{i=0}^{N} y_i \\ell_i(x),$$\n",
    "\n",
    "with the *Lagrange basis polynomials*  defined by the product\n",
    "\n",
    "$$\\ell_i(x) := \\prod_{\\begin{smallmatrix}0\\le m\\le N\\\\ m\\neq i\\end{smallmatrix}} \\frac{x-x_m}{x_i-x_m} = \\frac{(x-x_0)}{(x_i-x_0)} \\cdots \\frac{(x-x_{i-1})}{(x_i-x_{i-1})} \\frac{(x-x_{i+1})}{(x_i-x_{i+1})} \\cdots \\frac{(x-x_N)}{(x_i-x_N)},$$\n",
    "\n",
    "where $0\\le i\\le N$.\n",
    "\n",
    "\n",
    "An approximation to our integral is then given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I &\\approx \\int_a^b\\,P_N(x)\\,dx,\\\\[10pt]\n",
    "&= \\sum_{i=0}^{N}\\, \\left[\\,f(x_i) \\int_a^b\\, \\ell_i(x)\\, dx\\right],\\\\[10pt]\n",
    "&=\\sum_{i=0}^{N}\\, A_i \\, f(x_i),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ A_i := \\int_a^b\\, \\ell_i(x)\\, dx, \\;\\;\\;\\; i = 0,1,\\ldots,N. $$\n",
    "\n",
    "\n",
    "Simply by varying $N$ we can come up with quadrature rules of arbitrary order. \n",
    "\n",
    "Let's check we can recreate the schemes we have already seen via this approach.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### (N=1) Trapezoidal\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "If $N=1$ (i.e. two data points) then the Lagrange basis functions are \n",
    "\n",
    "$$\\ell_0(x) = \\frac{x-x_1}{x_0-x_1}= \\frac{x-b}{a-b}, \\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\ell_1(x) = \\frac{x-x_0}{x_1-x_0} = \\frac{x-a}{b-a},$$\n",
    "\n",
    "with integrals\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_0 & := \\int_a^b\\, \\ell_0(x)\\, dx =  \\frac{1}{a-b}\\int_a^b\\, (x-b)\\, dx  \\\\[5pt]\n",
    "&= \\frac{1}{a-b}\\left[\\frac{x^2}{2} - b x\\right]_a^b = \\frac{1}{a-b} \\left[\\frac{b^2 - a^2}{2} - b (b-a)\\right] \\\\[5pt]\n",
    "&= -\\frac 12 (a+b) + b = \\frac{b-a}2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_1 & := \\int_a^b\\, \\ell_1(x)\\, dx =  \\frac{1}{b-a}\\int_a^b\\, (x-a)\\, dx  \\\\[5pt]\n",
    "&= \\frac{1}{b-a}\\left[\\frac{x^2}{2} - a x\\right]_a^b = \\frac{1}{b-a} \\left[\\frac{b^2 - a^2}{2} -a (b-a)\\right] \\\\[5pt]\n",
    "&= \\frac 12 (b+a) - a = \\frac{b-a}2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and so our quadrature rule over a single interval can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I \\approx \\sum_{i=0}^{1}\\, A_i \\, f(x_i) = \n",
    "A_0 f(a) + A_1 f(b) = (b - a)\\frac{(f(a) + f(b))}{2},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which we indeed recognise as the Trapezoidal scheme from earlier.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### (N=2) Simpson's rule\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "\n",
    "If we go through the same steps with $N=2$ we arrive at the 1/3 Simpson rule from earlier [a homework exercise asks you to do this].\n",
    "\n",
    "[NB. We could also use the *method of undetermined coefficients* and Taylor Series - we will see this approach to constructing approximations to derivatives in a later Lecture.]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Observations\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "\n",
    "- Recall that fitting high-order polynomials through many (evenly spaced) data points was found to be a bad idea in the context of interpolation (we saw this in an example which used the Runge function). \n",
    "\n",
    "\n",
    "- This will of course also compromise the corresponding quadrature rules, and so we should be very careful about the use of high-order Newton-Cotes based quadrature.\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Open vs Closed Newton-Cotes formulae\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "Note that there are actually two types of Newton-Cotes formulae.\n",
    "\n",
    "\n",
    "1. \"Closed\" rules make use of function evaluations at the end points of an interval (e.g. all the schemes we have seen so far apart from midpoint).\n",
    "\n",
    "\n",
    "2. \"Open\" rules do not make use of the end points - so the midpoint rule would be an example of this type of rule amongst those that we've considered above.\n",
    "\n",
    "For more information on the rules and their error bounds see <http://mathworld.wolfram.com/Newton-CotesFormulas.html>\n",
    "\n",
    "There are sometimes advantages to not using the end points (i.e. an open formula) - for example sometimes the function being integrated is singular there, and yet the integrand is finite (see the example from the homework and corresponding convergence plot below).\n",
    "\n",
    "Also it is possible that for the same number of evaluation points an open rule may be more accurate - see the section on Gaussian quadrature below.\n",
    "\n",
    "For a discussion of open vs closed rules (and more generally the selection of a quadrature rule) see Section 4.12 of 'A First Course in Numerical Analysis', by Anthony Ralston and Philip Rabinowitz.\n",
    "\n",
    "One of the homework exercises asks you to implement an open rule (e.g. Milne's rule) and compare it with Simpson's rule to estimate the following problem that possesses a singularity at on the end points of the total integration interva.\n",
    "\n",
    "$$I := \\int_0^1 \\frac{1}{\\sqrt{x}}\\, dx.$$\n",
    "\n",
    "You should find an error plot that looks something like this\n",
    "\n",
    "```{figure} Figures/open_vs_closed_newton_cotes.png\n",
    ":width: 75%\n",
    "```\n",
    "\n",
    "We have lost our theoretical order of convergence for the open rule (and the closed rule of course gives garbage) - our analysis no longer applies since the function is singular at $x=0$, and our analysis based upon Taylor series does not hold for non-smooth functions (e.g. we cannot assume that higher-order terms in the Taylor series expansion can be ignored).  But still, the open rule is doing a whole lot better than the closed rule!\n",
    "   ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## (Richardson) extrapolation and Romberg integration  [$\\star\\star$]\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "Note in the above that to arrive at Simpson's rule we combined what we knew about the leading order terms in the errors for the Midpoint and Trapezoidal rules, and sought a new (more accurate) method by combining appropriate multiples of the results from the Midpoint and Trapezoidal rules to cancel the leading order errors.\n",
    "\n",
    "Similarly, we also used Simpson's rule with two different interval sizes to derive the more accurate Weddle's rule by seeking to cancel the leading order error terms [see homework].\n",
    "\n",
    "Note that we could also have used Trapezoidal with two interval sizes (one half the other) to arrive at Simpson's rule.\n",
    "\n",
    "This procedure of combining two estimates from a scheme with different interval or cell sizes is called [*Richardson extrapolation*](https://en.wikipedia.org/wiki/Richardson_extrapolation); we can apply it indefinitely to derive quadrature rules of any order.\n",
    "\n",
    "This idea can also be used within a recursive integration method which uses the Trapezoidal scheme at its core.  \n",
    "\n",
    "This method is termed [*Romberg's method*](https://en.wikipedia.org/wiki/Romberg%27s_method).\n",
    "\n",
    "Since the method operates recursively until an error tolerance is reached, the required number of function evaluations (and thus the locations where data is required) is not known a priori. \n",
    "\n",
    "Hence, the implementation in SciPy takes as an argument the function to be integrated itself. \n",
    "\n",
    "This is in contrast to the SciPy methods we have seen above which have generally taken the pre-computed function *values* at a fixed number of points, hence it is these values rather than the underlying function which are passed as arguments. Clearly this means that Romberg integration is not suitable for situations where we only have data points and no access to an underlying function.\n",
    "\n",
    "Let's call SciPy's Romberg integration function on our $f(x) = \\sin(x)$ problem to see what it does:\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# The argument 'show = True' means that it prints out its progress across steps/levels\n",
    "I_R = integrate.romberg(f, 0.0, np.pi, show=True)\n",
    "print(\"\\nRomberg integration error = {:.16e}\".format(abs(I_R-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Observations\n",
    "\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "- We achieved an error of around 1e-12 using 33 function evaluations.\n",
    "\n",
    "\n",
    "- Notice that for an error of ~1e-12 using `composite_simpsons_rule` we required around 1000 function evaluations; for an error of ~1e-12 with Weddle's rule we needed around 250 function evaluations.\n",
    "\n",
    "\n",
    "- A drawback of this approach is that it uses equally spaced evaluations of the function, which we know from earlier is highly problematic for certain functions when we try to go to high order. In addition it may be more efficient to space our intervals (or data points/function evaluations) in some sense based on how the variability of the function varies across our interval - this is covered next.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adaptive quadrature  [$\\star$]\n",
    "\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "\n",
    "We have noted above drawbacks of using equally spaced evaluations of the function, and going to too high an order of quadrature methods.\n",
    "\n",
    "Adaptive quadrature seeks to address this through the use of relatively low-order quadrature rules over subintervals that can vary in size, with the size based on how much variation there is at that location in the function being integrated.\n",
    "\n",
    "We will demonstrate the idea below, making use of Simpson's rule as the base low-order method.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Algorithm\n",
    "\n",
    "By the same argument that was used to derive Weddle's rule from Simpson's (and indeed similar to how we derived Simpson from Midpoint and Trapezoidal) we know that approximately\n",
    "\n",
    "\\begin{equation}\n",
    "16(I - I_{S_2}) \\approx I - I_{S} \\;\\;\\;\\; \\Rightarrow \\;\\;\\;\\;\n",
    "I - I_{S_2} \\approx \\frac {\\left(I_{S_2} - I_{S} \\right)}{15},\n",
    "\\end{equation}\n",
    "\n",
    "i.e. the error between the $I_{S_2}$ approximation of the integral and the true integral $I$ is approximately $1/15$ the difference in $I_{S_2}$ and $I_{S}$.\n",
    "\n",
    "Adaptive quadrature evaluates the integral with $I_S$ and $I_{S_2}$ over the entire interval $[a,b]$.\n",
    "\n",
    "If the error estimate provided by the formula above ($(I_{S_2} - I_{S})/15$) is below a *user-defined tolerance* then $I_{S_2}$ is returned as the numerical approximation to the integral (actually since we know $S$ and $S_2$ we might as well return the more accurate Weddle approximation: $I_W = I_{S_2} + (I_{S_2} - I_{S})/15$).\n",
    "\n",
    "Otherwise the interval is split into two and the same procedure is applied recursively to each of the two intervals, the final estimate of the integral being the sum of the recursively computed half intervals.\n",
    "\n",
    "Note that the recursive implementation computes explicitly $I_{S}$ from $a$ to the midpoint $c$ and adds $I_{S}$ from $c$ to $b$ to provide $I_{S_2}$ rather than calling the Simpson function with double the number of intervals, so that the $I_{S}$ from $a$ to $c$ or from $c$ to $b$ can be used as $I_{S}$ at the next recursive layer down.\n",
    "\n",
    "At every recursive layer down we halve the tolerance so that when summed the estimated total error is below our prescribed tolerance.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Implementation\n",
    "\n",
    "See the homework exercise for an implementation based upon iterative calls to our Simpson's rule function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Application to a simple problem\n",
    "\n",
    "Let's apply this idea to our simple function. The plot below demonstrates the locations of function evaluations.\n",
    "\n",
    "```{figure} Figures/adaptive_quadrature_simple_example.png\n",
    ":width: 75%\n",
    "```\n",
    "    \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "    \n",
    "It's pretty clear for this example that adaptive quadrature (based on Simpson's rule) is not going to beat Simpson's rule applied on equally spaced intervals.\n",
    "\n",
    "But the story should be different for a more complex function with some regions of rapid variation, and others with more sedate variation - let's see an example.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Application to a complex problem\n",
    "\n",
    "We see in the plot below a far more complex function where we really are making good use of the ability to evaluate the function in an adaptive manner.\n",
    "\n",
    "```{figure} Figures/daptive_quadrature_example.png\n",
    ":width: 75%\n",
    "```\n",
    "\n",
    "<br>\n",
    "    \n",
    "Again, this plot is showing the non-uniform, or adaptive, locations at which the function has been evaluated by the algorith.\n",
    "\n",
    "Clearly this should help with the efficiency of the method, but let's check this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above example uses the following more complex function \n",
    "# (taken from Moin which quotes the exact integral as 0.56681975015)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate.\n",
    "    \n",
    "    This a more complicated example which has been taken from the book by Moin.\n",
    "    \"\"\"\n",
    "    return (10*np.exp(-50*np.abs(x)) -\n",
    "            (0.01/(np.power(x-0.5, 2) + 0.001)) +\n",
    "            5*np.sin(5*x))\n",
    "\n",
    "# refer to the homework for the code which actually integrates this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrature method comparison\n",
    "\n",
    "Let's compare errors vs number of function evaluations for the more complicated problem used to demonstrate adaptive quadrature. \n",
    "\n",
    "Recreating this comparison/graph is given in the homework solutions that asks you to implement adaptive quadrature.\n",
    "\n",
    "```{figure} Figures/compare_quad_methods_complex_function.png\n",
    ":width: 75%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Note that given the higher-order Weddle rule isn't obviously better than Simpson's rule for this function, at least at moderate error levels and numbers of function evaluations, it isn't a surprise that Romberg integration doesn't do brilliantly for this complex example either. \n",
    "\n",
    "However,  this is clearly a problem which is well-suited for adaptive quadrature, as we see in the plot of the function and locations of function evaluations above - for this problem the method really is taking advantage of the ability to vary the subinterval sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Gaussian quadrature  [$\\star\\star$]\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "The quadrature rules introduced above all had the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I = \\int_a^b\\,f(x)\\,dx \\approx \\sum_{i=0}^{N}\\, A_i \\, f(x_i),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the *nodes* (or *abscissas*) $x_i$ were fixed in advance (e.g. uniformly spaced with Newton-Cotes), and with the weights $A_i$ then chosen such that polynomials up to a certain order could be integrated exactly.\n",
    "\n",
    "In particular, for Newton-Cotes we used Lagrange interpolation such that with our $N+1$ free parameters, $A_i$, we were able to integrate exactly polynomials of order *N* (or order $N+1$ in the case of even $N$).\n",
    "\n",
    "The idea behind Gaussian quadrature is to start from the same formula as above, but to consider both the weights **and the nodes** as free parameters with which to formulate quadrature rules.\n",
    "\n",
    "We thus have $2N+2$ free parameters in total which can be chosen to integrate polynomials of order $2N+1$ exactly.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Gauss-Legendre quadrature\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "To action this process for a polynomial of order $2N+1$ we can simply require that the individual powers (i.e. the monomials) from 0 up to $2N + 1$ are integrated exactly:\n",
    "\n",
    "$$\\int_{-1}^{1} x^j \\,dx = \\sum_{i=0}^{N}\\, A_i \\, f(x_i) = \\sum_{i=0}^{N}\\, A_i \\, x_i^j\n",
    "\\;\\;\\;\\;\\;\\;\\text{for}\\;\\;\\;\\;\\;\\;  j = 0, 1, \\dots, 2 N + 1.$$\n",
    "\n",
    "    \n",
    "This will provide us with $2N+2$ pieces of information with which to figure out the $2N+2$ coefficients needed to define the method.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "In the case of $N=1$, for example, we can perform these integrals.\n",
    "\n",
    "\n",
    "- For $\\;j = 0$:\n",
    "\n",
    "$$LHS = \\int_{-1}^{1} x^0 \\,dx = \\int_{-1}^{1} 1 \\,dx = 2,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$RHS = \\sum_{i=0}^{N}\\, A_i \\, x_i^0 = A_0 + A_1. $$\n",
    "\n",
    "\n",
    "- For $\\;j = 1$:\n",
    "\n",
    "$$LHS = \\int_{-1}^{1} x^1 \\,dx = \\left. \\frac{1}{2}x^2 \\right|_{-1}^1 = 0,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$RHS = \\sum_{i=0}^{N}\\, A_i \\, x_i^1 = A_0x_0 + A_1x_1, $$\n",
    "\n",
    "and so on, to yield the four simultaneous equations for our four free parameters $\\{x_0, x_1, A_0, A_1\\}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "j & = 0  & A_0 + A_2 & = 2 \\\\[5pt]\n",
    "j & = 1  & A_0 x_0 + A_2 x_1 & = 0 \\\\[5pt]\n",
    "j & = 2  & A_0 x_0^2 + A_2 x_1^2 & = \\frac{2}{3}\\\\[5pt]\n",
    "j & = 3  & A_0 x_0^3 + A_2 x_1^3 & = 0,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which we can solve to yield: \n",
    "\n",
    "$$x_0 = -\\frac{1}{\\sqrt{3}}, \\;\\;\\; x_1 = \\frac{1}{\\sqrt{3}}, \\;\\;\\;\n",
    "A_0 = 1,\\;\\;\\; A_1 = 1,$$\n",
    "\n",
    "and so our quadrature rule is\n",
    "\n",
    "$$\\int_{-1}^1 f(x)\\,dx \\approx f\\left( -\\frac{1}{\\sqrt{3}} \\right) +  f\\left( \\frac{1}{\\sqrt{3}} \\right).$$\n",
    "  \n",
    "This is termed a *Gauss-Legendre* quadrature rule, as are all methods we can derive in this manner, with differing order based on the choice of $N$ and starting from the requirement that \n",
    "\n",
    "$$\\int_{-1}^{1} x^j \\,dx = \\sum_{i=0}^{N}\\, A_i \\, f(x_i) = \\sum_{i=0}^{N}\\, A_i \\, x_i^j, \\;\\;\\;\\; \\text{for} \\;\\;\\;\\; j = 0, 1, \\dots, 2 N + 1.$$\n",
    "\n",
    "For a table of rules see <https://en.wikipedia.org/wiki/Gaussian_quadrature#Gauss%E2%80%93Legendre_quadrature>.\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "A nice description of the process more generally can be found here [https://rosettacode.org/wiki/Numerical_integration/Gauss-Legendre_Quadrature](https://rosettacode.org/wiki/Numerical_integration/Gauss-Legendre_Quadrature)\n",
    "\n",
    "We can use SciPy to compute either fixed order [scipy.integrate.fixed_quad](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.fixed_quad.html#scipy.integrate.fixed_quad) or fixed tolerance [scipy.integrate.quadrature](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.quadrature.html) Gaussian quadrature.\n",
    "\n",
    "We can also access the Gauss-Legendre quadrature rule using [numpy.polynomial.legendre.leggauss](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.legendre.leggauss.html) as we see below. \n",
    "\n",
    "NB. we can check that the $x$ values for degree 2 are indeed the $\\pm 1/\\sqrt{3}$ we calculated above:\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Gauss-Legendre nodes and weights\n",
    "degree = 2\n",
    "xi, w = np.polynomial.legendre.leggauss(degree)\n",
    "print('Nodes, xi: ',xi)\n",
    "print('Weights, w: ',w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default print precision is 8 - let's get more s.f.s\n",
    "np.set_printoptions(precision=16)\n",
    "degrees = np.linspace(1, 4 , 4, dtype=int)\n",
    "for degree in degrees:\n",
    "    x, w = np.polynomial.legendre.leggauss(degree)\n",
    "    print('\\ndegree {0:1d}: \\n x = {1} \\n w = {2}'.format(degree, str(x), str(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Changing integration limits\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "Note that the above derivation, and hence the locations of the nodes, were with respect to an integration limit of $[-1,1]$.\n",
    "\n",
    "In order to evaluate an integral over a general interval $[a,b]$ a change of coordinates is required, but this it not difficult to come up with or implement:\n",
    "\n",
    "An appropriate transformation between $x\\in[a,b]$ and $\\xi\\in[-1,1]$ is:\n",
    "\n",
    "$$x = \\frac{b+a}{2} + \\frac{b-a}{2}\\xi,$$\n",
    "\n",
    "with the weights $\\xi_i$ then given from the process above, e.g. \n",
    "\n",
    "$$\\xi_0 = -\\frac{1}{\\sqrt{3}}, \\;\\;\\; \\xi_1 = \\frac{1}{\\sqrt{3}},$$\n",
    "\n",
    "the desired integral can then be evaluated via\n",
    "\n",
    "$$\\int_a^b f(x)\\,dx = \\frac{b-a}{2} \\sum_{i=0}^{N}\\, A_i \\, f\\left( \\frac{b+a}{2} + \\frac{b-a}{2}\\xi_i \\right).$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's use this knowledge to write our own Gauss-Legendre quadrature function over a single interval and apply it to our simple example function, comparing our results to the SciPy function `scipy.integrate.fixed_quad`.\n",
    "\n",
    "Note that the implementation of a composite version of Gauss-Legendre is a homework exercise.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function we wish to integrate\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# interval to integrate over\n",
    "a = 0.0\n",
    "b = np.pi\n",
    "\n",
    "def gauss_legendre(a, b, function, degree=5):\n",
    "    \"\"\"Function to evaluate Gauss Legendre quadrature\n",
    "    \"\"\"\n",
    "    # compute the Gauss-Legendre nodes and weights\n",
    "    xi, w = np.polynomial.legendre.leggauss(degree)\n",
    "\n",
    "    # Translate the node locations from the interval [-1, 1] to [a, b]\n",
    "    x =  0.5*(b + a) + 0.5 * (b - a) * xi\n",
    " \n",
    "    return 0.5*(b - a) * sum(w * f(x))\n",
    "\n",
    "\n",
    "degrees = np.linspace(1, 6 , 6, dtype=int)\n",
    "for degree in degrees:\n",
    "    I_gl =  gauss_legendre(a, b, f, degree)\n",
    "    I_sigl = integrate.fixed_quad(f, a, b, n=degree)[0]\n",
    "    print('Integrals from our code and SciPy for degree {0} = {1}, {2}'.format(degree,I_gl,I_sigl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Other Gaussian quadrature families\n",
    "\n",
    "```{toggle} Advanced Content\n",
    ":show:\n",
    "\n",
    "We obtained the *Gauss-Legendre* family of quadrature rules from the requirement that \n",
    "\n",
    "\n",
    "$$\\int_{-1}^{1} x^j \\,dx = \\sum_{i=0}^{N}\\, A_i \\, f(x_i) = \\sum_{i=0}^{N}\\, A_i \\, x_i^j, \\;\\;\\;\\; \\text{for} \\;\\;\\;\\; j = 0, 1, \\dots, 2 N + 1.$$\n",
    "\n",
    "\n",
    "<br>    \n",
    "    \n",
    "If we start the exercise again, but assume we wish to evaluate integrals taking the form\n",
    "\n",
    "$$\\int_{-1}^{1} w(x)f(x)\\,dx,$$\n",
    "\n",
    "as accurately as possible, where $w(x)$ is a weighting function, then\n",
    "different choices of weighting function lead us to other families of quadrature rules which are each useful for accurately integrating different types of commonly encountered functions.\n",
    "\n",
    "[We arrived at the Gauss-Legendre quadrature rules above through the simplest choice of $w(x)\\equiv 1$.]\n",
    "\n",
    "The idea and motivation behind this is that some functions $F(x)$ which are not themselves close to, or well represented by, polynomials can be rewritten in the form $F(x) = w(x)f(x)$ where $f(x)$ *is* close to polynomial and $w(x)$ is from a known family of functions.\n",
    "\n",
    "For more information see [https://en.wikipedia.org/wiki/Gaussian_quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature)\n",
    "\n",
    "We can generalise our `gauss_legendre` code above to implement rules with standard weighting functions by accessing the appropriate pre-computed nodes and weights using, e.g.,\n",
    "[`numpy.polynomial.chebyshev.chebgauss`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.chebyshev.chebgauss.html#numpy.polynomial.chebyshev.chebgauss) or [`numpy.polynomial.laguerre.laggauss`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.laguerre.laggauss.html#numpy.polynomial.laguerre.laggauss), etc.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Higher dimensions  [$\\star$]\n",
    "\n",
    "```{toggle} Optional Content\n",
    ":show:\n",
    "\n",
    "In higher dimensions similar ideas to those presented above can also be applied.\n",
    "\n",
    "For further reading see <https://en.wikipedia.org/wiki/Numerical_integration#Multidimensional_integrals>.\n",
    "\n",
    "Note that sometimes higher dimensional quadrature is called \"cubature\": <http://mathworld.wolfram.com/Cubature.html>.\n",
    "\n",
    "For a SciPy method see <https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.nquad.html>.\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "- We have taken the opportunity to revisit interpolation/the approximation of functions, and covered some new methods...\n",
    "\n",
    "\n",
    "- in particular the use of \"hat\" functions as a basis for p/w linear interpolation.\n",
    "\n",
    "\n",
    "- We have also revisited quadrature ...\n",
    "\n",
    "\n",
    "- covering more on error analysis,\n",
    "\n",
    "\n",
    "- and in particular further families of quadrature methods, including adaptive algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you need to focus on?\n",
    "\n",
    "\n",
    "- General familiarity with errors and their dependence on the specifics of the numerical method, the function being considered and the interval size (or mesh resolution).\n",
    "\n",
    "\n",
    "- The specifics of the hat functions used as a basis for p/w linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
